# 蒙特卡洛方法

## 面试摘要

**蒙特卡洛（MC）方法** 从完整的回合中学习价值函数，不需要模型。关键思想：对访问过的状态的回报取平均。两种变体：**首次访问MC**（每个回合只计算第一次出现）和**每次访问MC**（所有出现）。MC有零偏差但高方差。只适用于回合任务。形成了DP和TD学习之间的概念桥梁。

**需要记忆的**：MC更新规则，首次访问vs每次访问，高方差/零偏差，探索起点vs ε-软策略。

---

## 核心定义

### 蒙特卡洛估计

**思想**：状态的价值是期望回报。通过对观测到的回报取平均来估计它。

$$V(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)$$

其中 \(G_i(s)\) 是第 \(i\) 个回合中访问 \(s\) 后观测到的回报。

### 首次访问 vs 每次访问

- **首次访问MC**：每个回合只计算第一次访问 \(s\)
- **每次访问MC**：计算每次访问 \(s\)

当 \(N(s) \to \infty\) 时，两者都收敛到 \(V^\pi(s)\)。首次访问有稍好的理论性质（无偏估计器）。

### 增量均值更新

不需要存储所有回报：

$$V(s) \leftarrow V(s) + \alpha \left[ G - V(s) \right]$$

其中 \(\alpha = \frac{1}{N(s)}\) 用于精确平均，或对于非平稳环境使用固定 \(\alpha\)。

---

## 数学与推导

### 为什么MC是无偏的

$$\mathbb{E}[G_t | S_t = s] = V^\pi(s)$$

根据 \(V^\pi\) 的定义。MC直接采样 \(G_t\)，所以平均值收敛到期望。

### MC估计的方差

$$\text{Var}(G_t) = \text{Var}\left( \sum_{k=0}^{T-t} \gamma^k R_{t+k+1} \right)$$

方差随回合长度增长。长回合 → 高方差 → 学习缓慢。

**与TD对比**：TD从 \(V(s')\) 自举，这减少了方差但引入了偏差（如果 \(V\) 不正确）。

### Q值的MC

学习Q值（用于无模型控制）：

$$Q(s,a) \approx \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G_i(s,a)$$

需要访问所有 \((s,a)\) 对 → 需要探索。

---

## 算法概述

### MC预测（策略评估）

```
算法：首次访问MC预测

输入：策略 π，回合数
输出：V^π

1. 任意初始化 V(s)，Returns(s) = [] 对所有 s
2. 对每个回合：
     生成回合：S_0, A_0, R_1, ..., S_T 遵循 π
     G = 0
     对 t = T-1, T-2, ..., 0：
         G = γG + R_{t+1}
         如果 S_t 不在 {S_0, ..., S_{t-1}} 中：  # 首次访问检查
             将 G 添加到 Returns(S_t)
             V(S_t) = mean(Returns(S_t))
3. 返回 V
```

### MC控制（寻找最优策略）

```
算法：带 ε-软策略的MC控制

输入：ε，回合数
输出：近似最优 π

1. 任意初始化 Q(s,a)，π = 相对于 Q 的 ε-贪婪
2. 对每个回合：
     生成遵循 π 的回合
     G = 0
     对 t = T-1, T-2, ..., 0：
         G = γG + R_{t+1}
         使用 G 更新 Q(S_t, A_t)
     π = 相对于 Q 的 ε-贪婪
3. 返回 π
```

### 探索起点

ε-软的替代方案：从随机的 (s,a) 对开始回合。确保所有对都被访问。在真实环境中不实用。

---

## 常见陷阱

1. **对连续任务应用MC**：MC需要完整的回合。无限任务没有回合结束。

2. **探索不足**：MC需要访问所有状态-动作对。没有探索，一些对永远不会被更新。

3. **长回合的高方差**：回报累积所有步骤的噪声。考虑对长回合使用TD。

4. **忘记处理终止状态**：回报不应该包括终止状态之后的任何内容。

5. **首次访问使用固定α**：如果使用增量更新，\(\alpha = 1/N(s)\) 用于精确平均。

---

## 小例子

**简单回合：**
```
S0 → (R=0) → S1 → (R=0) → S2 → (R=1) → 终止
```

当 \(\gamma = 0.9\) 时：
- \(G(S2) = 1\)
- \(G(S1) = 0 + 0.9 \times 1 = 0.9\)
- \(G(S0) = 0 + 0.9 \times 0.9 = 0.81\)

**1个回合后**：
- \(V(S0) = 0.81\)，\(V(S1) = 0.9\)，\(V(S2) = 1\)

**100个回合后**（每次相同轨迹）：
- 值保持不变（低方差因为轨迹是确定性的）

**随机转移时**：经过许多回合后值收敛到期望。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> MC相对于DP的关键优势是什么？</summary>

**答案**：MC不需要模型（转移概率 \(P(s'|s,a)\)）。它直接从经验中学习。

**解释**：DP需要完整的模型来计算期望。MC从环境中采样轨迹，所以它只需要能够交互（不需要知道动态）。

**关键洞见**：这使MC适用于模型不可用的真实世界问题。

**常见陷阱**：MC仍然需要完整的回合，这是它自己的限制。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么MC相比TD有高方差？</summary>

**答案**：MC使用完整回报 \(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)，这是许多随机变量的总和。每个随机奖励都增加方差。

**解释**：总和的方差随项数增长。TD使用 \(R_{t+1} + \gamma V(S_{t+1})\)，这只是一个随机奖励加上一个学习的估计。估计 \(V(S_{t+1})\) 有偏差但方差更低。

**关键方程**：\(\text{Var}(G) \gg \text{Var}(R + \gamma V)\) 当回合很长时。

**常见陷阱**：高方差意味着收敛需要更多回合，而不是MC是错误的。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 推导MC的增量更新规则。</summary>

**答案**：从均值更新开始：

\(n\) 次访问后，\(V_n = \frac{1}{n} \sum_{i=1}^{n} G_i\)。

\(n+1\) 次访问后：

$$V_{n+1} = \frac{1}{n+1} \sum_{i=1}^{n+1} G_i = \frac{1}{n+1} \left( nV_n + G_{n+1} \right)$$

$$= V_n + \frac{1}{n+1}(G_{n+1} - V_n)$$

带学习率 \(\alpha\) 的一般形式：

$$V(s) \leftarrow V(s) + \alpha [G - V(s)]$$

**解释**：更新将 \(V(s)\) 向 \(G\) 移动分数 \(\alpha\)。当 \(\alpha = 1/n\) 时，这计算精确平均。

**常见陷阱**：固定 \(\alpha\) 不计算真正的平均——它更重视最近的回报。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 为什么MC控制必须使用ε-软或探索起点？</summary>

**答案**：确保所有状态-动作对被无限次访问，保证收敛。

**解释**：MC通过平均 (s,a) 后的回报来估计 Q(s,a)。如果我们从不访问 (s,a)，我们就无法估计它。确定性策略可能永远不会访问某些对 → 这些对没有学习。

**关键要求**：GLIE（极限贪婪且无限探索）：无限探索所有对，但最终变得贪婪。

**常见陷阱**：没有探索，MC控制可能收敛到次优策略，因为它永远发现不了更好的动作。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> MC学习对你的环境非常慢。什么可能有帮助？</summary>

**答案**：可能的改进：
1. **切换到TD**：更低方差，在回合中学习
2. **回合截断**：限制回合长度，将截断视为终止
3. **基线减法**：使用优势而不是原始回报
4. **增加批量大小**：每次更新对更多回合取平均
5. **奖励塑形**：添加中间奖励以减少时间范围

**解释**：由于方差，MC的样本复杂度是 \(O(\text{回合长度}^2)\)。带自举的TD通常快几个数量级。

**常见陷阱**：假设问题需要MC。在大多数情况下，TD或n步方法更可取。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第5章
- **Singh & Sutton (1996)**, 带替换资格迹的强化学习
- **Kakade (2001)**, 自然策略梯度

**面试需要记忆的**：MC更新规则，首次访问vs每次访问，无偏但高方差，需要回合，探索起点或ε-软。

**代码示例**：[mc_control.py](../../rl_examples/algorithms/mc_control.py)
