# Q学习

## 面试摘要

**Q学习** 是一个异策略TD控制算法，直接学习 \(Q^*\)。关键洞见：在目标中使用 \(\max_{a'} Q(s', a')\)，不管实际采取了什么动作。这允许在遵循探索性策略的同时学习最优值。RL中最重要的算法之一——DQN的基础。

**需要记忆的**：Q学习更新规则，异策略性质（行为策略vs目标策略），收敛到 \(Q^*\)，与SARSA的比较。

---

## 核心定义

### Q学习更新规则

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \right]$$

**与SARSA的关键区别**：使用 \(\max_{a'}\) 而不是实际采取的动作。

### 异策略学习

- **行为策略**（\(b\)）：用于生成动作的策略（例如ε-贪婪）
- **目标策略**（\(\pi\)）：正在学习的策略（相对于Q贪婪）

Q学习在遵循覆盖所有动作的任何行为策略 \(b\) 的同时学习贪婪策略 \(\pi^*\)。

### TD目标

$$\text{目标} = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')$$

这是在一个转移处采样的贝尔曼最优方程。

---

## 数学与推导

### 与贝尔曼最优的关系

Q学习近似：

$$Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') | S_t=s, A_t=a]$$

每次更新是朝向这个不动点的基于样本的步骤。

### 收敛定理

**定理**（Watkins & Dayan, 1992）：如果满足以下条件，Q学习以概率1收敛到 \(Q^*\)：
1. 所有状态-动作对被无限次访问
2. 学习率满足：\(\sum_t \alpha_t = \infty\)，\(\sum_t \alpha_t^2 < \infty\)
3. 奖励有界

**注意**：不管行为策略如何都收敛到 \(Q^*\)！

### 为什么异策略有效

max算子将学习与行为解耦：
- 我们从任何策略采样 \((s, a, r, s')\)
- 我们向最优更新，不管我们如何到达那里
- 只要我们访问所有对，我们就学习所有对

---

## 算法概述

```
算法：Q学习（异策略TD控制）

输入：α, γ, ε
输出：Q ≈ Q*

1. 任意初始化 Q(s,a)（Q(terminal, ·) = 0）
2. 对每个回合：
     S = 初始状态
     当 S 不是终止状态时：
         A = 从 Q(S, ·) 的 ε-贪婪动作
         执行动作 A，观察 R, S'
         Q(S,A) ← Q(S,A) + α[R + γ max_a' Q(S',a') - Q(S,A)]
         S ← S'
3. 返回 Q
```

**简单性**：注意我们不需要在更新前选择 \(A'\)——我们只是计算max。

---

## 常见陷阱

1. **过估计偏差**：max算子倾向于过估计Q值（见Double Q学习）。

2. **仍需要探索**：异策略不意味着不需要探索！必须仍然访问所有 (s,a) 对。

3. **函数逼近不稳定**：带函数逼近的Q学习可能发散（致命三角）。

4. **与SARSA混淆**：Q学习使用max；SARSA使用实际下一个动作。

5. **忽视行为策略要求**：行为策略必须覆盖所有动作才能收敛。

---

## 小例子

**简单网格：**
```
+---+---+
| S | G |
+---+---+
```
动作：左、右。奖励：G处+1，否则0。\(\gamma = 0.9\)，\(\alpha = 0.5\)。

**回合1**（行为：随机）：
- S，右，R=1，G（终止）
- 更新：\(Q(S, 右) = 0 + 0.5[1 + 0.9 \times 0 - 0] = 0.5\)

**回合2**（行为：ε-贪婪）：
- S，右，R=1，G（终止）— 贪婪选择了右
- 更新：\(Q(S, 右) = 0.5 + 0.5[1 + 0 - 0.5] = 0.75\)

**回合3**：
- \(Q(S, 右) = 0.75 + 0.5[1 - 0.75] = 0.875\)

**收敛到**：\(Q^*(S, 右) = 1\)，\(Q^*(S, 左) = 0.9\)（如果左循环回到S）。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 什么使Q学习成为"异策略"？</summary>

**答案**：TD目标使用 \(\max_{a'} Q(s', a')\)，假设接下来会采取贪婪动作——不管行为策略实际选择了什么动作。

**解释**：异策略意味着正在评估/改进的策略（目标策略=贪婪）与生成数据的策略（行为策略=探索性）不同。max将这些解耦。

**关键洞见**：这允许在探索的同时学习最优值。

**常见陷阱**：认为异策略意味着"不需要探索"。你仍然需要探索来访问所有状态！
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么Q学习有过估计偏差？</summary>

**答案**：对带噪声Q估计取max倾向于选择被过估计的值。

**解释**：如果 \(Q(s', a_1)\) 因为噪声被过估计，\(\max_a Q(s', a)\) 会选择它。经过多次更新，这个偏差累积。即使无偏噪声也导致有偏的max。

**关键方程**：\(\mathbb{E}[\max_i X_i] \geq \max_i \mathbb{E}[X_i]\)（类似Jensen的max不等式）。

**解决方案**：Double Q学习使用两个Q函数来解耦选择和评估。

**常见陷阱**：在DQN中忽视这个导致过估计的值和次优策略。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 比较Q学习和SARSA的更新规则。它们何时不同？</summary>

**答案**：
- **Q学习**：\(Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma \max_{a'} Q(S',a') - Q(S,A)]\)
- **SARSA**：\(Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)]\)

当 \(A' \neq \arg\max_{a'} Q(S', a')\) 时它们不同，即当行为策略没有选择贪婪动作时。

**解释**：用ε-贪婪，这在ε比例的时间发生。用确定性贪婪，它们是相同的。

**关键洞见**：当 ε → 0，SARSA → Q学习。但在学习期间，差异很重要。

**常见陷阱**：认为它们是名字不同的相同算法。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 陈述Q学习的收敛条件。</summary>

**答案**：Q学习以概率1收敛到 \(Q^*\) 如果：
1. 所有 (s,a) 对被无限次访问
2. 学习率：\(\sum_t \alpha_t = \infty\) 且 \(\sum_t \alpha_t^2 < \infty\)
3. 奖励有界
4. MDP是有限的

**解释**：条件1确保所有对都被学习。条件2确保我们能到达目标（\(\sum \alpha = \infty\)）但停在那里（\(\sum \alpha^2 < \infty\) 限制噪声）。

**常见学习率**：\(\alpha_t = 1/t\) 或 \(\alpha_t = 1/t^{0.8}\)。

**常见陷阱**：常数 \(\alpha\) 不满足条件2——没有收敛保证（但实践中通常有效）。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> Q学习带函数逼近在发散。为什么？</summary>

**答案**："致命三角"——以下组合：
1. 函数逼近（泛化）
2. 自举（在目标中使用估计）
3. 异策略学习

这个组合可以导致不稳定和发散。

**解释**：函数逼近意味着更新一个状态影响其他状态。自举复合误差。异策略意味着数据分布与我们评估的不匹配。

**解决方案**：
- 目标网络（DQN）
- 带优先级的经验回放
- 软目标更新
- 正则化

**常见陷阱**：假设表格收敛保证延伸到函数逼近。
</details>

---

## 参考文献

- **Watkins & Dayan (1992)**, Q-Learning — 原始收敛证明
- **Sutton & Barto**, 强化学习：导论，第6.5章
- **Mnih et al. (2015)**, 通过深度RL实现人类水平控制 — DQN论文

**面试需要记忆的**：带max的更新规则，异策略定义，收敛条件，过估计偏差，致命三角。

**代码示例**：[q_learning.py](../../../rl_examples/algorithms/q_learning.py)
