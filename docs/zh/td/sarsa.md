# SARSA

## 面试摘要

**SARSA**（状态-动作-奖励-状态-动作）是一个同策略TD控制算法。它使用元组 \((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\) 更新Q值——因此得名。更新使用实际采取的动作，使其成为同策略的。在GLIE条件下收敛到最优Q。比Q学习更保守，因为它考虑了自己的探索。

**需要记忆的**：SARSA更新规则，同策略性质，收敛到 \(Q^\pi\)（除非探索消失否则不是 \(Q^*\)），与Q学习的比较。

---

## 核心定义

### SARSA更新规则

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]$$

**组成部分**：
- \(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})\)：TD目标（我们向其移动）
- \(Q(S_t, A_t)\)：当前估计
- \(\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\)：TD误差

### 同策略

目标 \(Q(S_{t+1}, A_{t+1})\) 使用实际采取的动作 \(A_{t+1}\)（从策略采样）。这意味着SARSA学习正在遵循的策略的价值，包括探索。

### 为什么叫"SARSA"？

名字来自所需的五元组：\((S, A, R, S', A')\)。

---

## 数学与推导

### 与贝尔曼方程的关系

SARSA是以下方程的基于采样的近似：

$$Q^\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma Q^\pi(S_{t+1}, A_{t+1}) | S_t=s, A_t=a]$$

每次更新是朝向这个不动点的随机梯度步。

### 收敛性

在标准条件下（GLIE策略，适当的学习率衰减），SARSA收敛：

$$Q(s,a) \to Q^\pi(s,a)$$

如果 \(\pi\) 在极限中变得贪婪，那么 \(Q \to Q^*\)。

### TD误差

$$\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$

TD误差衡量惊讶程度：实际回报与我们期望的有多大不同？

---

## 算法概述

```
算法：SARSA（同策略TD控制）

输入：α, γ, ε
输出：Q ≈ Q*

1. 任意初始化 Q(s,a)（Q(terminal, ·) = 0）
2. 对每个回合：
     S = 初始状态
     A = 从 Q(S, ·) 的 ε-贪婪动作
     当 S 不是终止状态时：
         执行动作 A，观察 R, S'
         A' = 从 Q(S', ·) 的 ε-贪婪动作
         Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]
         S ← S', A ← A'
3. 返回 Q
```

**关键点**：\(A'\) 在更新之前选择，使用生成 \(A\) 的同一策略。

---

## 常见陷阱

1. **与Q学习混淆**：SARSA使用来自策略的 \(A'\)；Q学习使用 \(\max_{a'}\)。不同的算法！

2. **更新前不选择A'**：你必须选择 \(A'\) 来计算目标。不要先更新再选择。

3. **终止状态处理**：当 \(S'\) 是终止状态时，使用 \(Q(S', A') = 0\)。

4. **期望带探索的Q***：当固定 \(\epsilon > 0\) 时，SARSA收敛到 \(Q^\epsilon\)（ε-贪婪策略的价值），不是 \(Q^*\)。

5. **学习率**：太高 → 振荡。太低 → 慢。随时间衰减以保证收敛。

---

## 小例子

**悬崖行走：**

```
+---+---+---+---+
| S |   |   | G |
+---+---+---+---+
| C | C | C | C |   C = 悬崖（R = -100，返回 S）
+---+---+---+---+
```

奖励：每步-1，悬崖-100。

**SARSA with ε = 0.1**：学习走安全路径（顶行），因为它考虑了偶尔随机掉入悬崖的情况。

**Q学习**：学习走悬崖边缘，因为它假设会采取最优（贪婪）动作。

**关键洞见**：SARSA更保守——它学习它实际做的事情的价值（包括错误）。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 为什么SARSA被称为"同策略"？</summary>

**答案**：因为它使用当前策略下实际采取的动作 \(A'\) 来计算TD目标。

**解释**：目标 \(R + \gamma Q(S', A')\) 反映了策略接下来实际会做什么（包括探索）。这意味着SARSA学习行为策略的 \(Q^\pi\)，而不是最优的 \(Q^*\)。

**对比**：Q学习使用 \(\max_{a'} Q(S', a')\) 不管实际采取了什么动作——它是异策略的。

**常见陷阱**：当固定 \(\epsilon\) 时，SARSA收敛到ε-贪婪策略的价值，由于探索这比最优低。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 在悬崖行走中，为什么SARSA学习比Q学习更安全的路径？</summary>

**答案**：SARSA学习ε-贪婪策略的价值，其中包括偶尔随机掉入悬崖。安全路径在这个策略下的期望惩罚更低。

**解释**：SARSA评估"如果我继续使用这个探索性策略会发生什么？"在悬崖附近，随机动作有时会掉下去。Q学习评估"假设完美执行什么是最优的？"——它忽略探索。

**关键洞见**：当被学习的策略也在被执行时，SARSA更稳健。

**常见陷阱**：认为Q学习的路径是"错误的"——它对贪婪策略是最优的，只是在学习期间有风险。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 写出SARSA更新并解释为什么A'必须从策略中选择。</summary>

**答案**：

$$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)]$$

\(A'\) 必须来自策略因为SARSA估计：

$$Q^\pi(s,a) = \mathbb{E}_\pi[R + \gamma Q^\pi(S',A')]$$

期望是对 \(A' \sim \pi(\cdot|S')\) 取的。通过从 \(\pi\) 采样 \(A'\)，我们得到无偏估计。

**关键点**：如果 \(A'\) 来自不同的策略，我们就在估计 \(Q^\pi\) 以外的东西。

**常见陷阱**：贪婪地选择 \(A'\) 会使它变成异策略的（像Q学习）。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> SARSA的收敛与Q学习有何不同？</summary>

**答案**：
- **SARSA**：收敛到 \(Q^\pi\)，其中 \(\pi\) 是行为策略（例如ε-贪婪）。只有当 \(\epsilon \to 0\)（GLIE）时才收敛到 \(Q^*\)。
- **Q学习**：直接收敛到 \(Q^*\)，不管行为策略是什么（只要所有对都被访问）。

**解释**：SARSA的目标取决于将采取什么动作；Q学习的目标假设最优动作。SARSA需要GLIE；Q学习不需要。

**关键方程差异**：
- SARSA：\(R + \gamma Q(S', A')\) 其中 \(A' \sim \pi\)
- Q学习：\(R + \gamma \max_{a'} Q(S', a')\)

**常见陷阱**：假设SARSA总是找到最优策略——它找到极限策略的最优。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> SARSA在振荡且不收敛。出了什么问题？</summary>

**答案**：可能的问题：
1. **学习率太高**：尝试 \(\alpha = 0.1\) 或更低
2. **学习率没有衰减**：需要 \(\sum \alpha = \infty\)，\(\sum \alpha^2 < \infty\)
3. **ε没有衰减**：固定探索阻止收敛到最优
4. **环境是随机的**：可能需要更多回合
5. **初始化问题**：差的初始Q可以减慢收敛

**解释**：收敛需要适当的学习率计划。常见选择：\(\alpha_n = \frac{1}{n^{0.8}}\) 或类似的。

**常见陷阱**：使用常数 \(\alpha = 1.0\)——这不会收敛，只会在解周围振荡。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第6.4章
- **Rummery & Niranjan (1994)**, 使用连接主义系统的在线Q学习
- **Singh et al. (2000)**, 单步同策略RL算法的收敛结果

**面试需要记忆的**：SARSA更新，同策略定义，悬崖行走与Q学习的比较，收敛到 \(Q^\pi\)。

**代码示例**：[sarsa.py](../../../rl_examples/algorithms/sarsa.py)
