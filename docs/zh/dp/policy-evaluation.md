# 策略评估

## 面试摘要

**策略评估** 计算给定策略 \(\pi\) 的 \(V^\pi(s)\)。它迭代应用贝尔曼期望方程直到收敛。这是策略迭代中的"E"步骤。每次迭代的复杂度是 \(O(|S|^2|A|)\)。关键洞见：我们通过不动点迭代来求解线性方程组。

**需要记忆的**：迭代更新规则，\(\gamma < 1\) 时的收敛保证，与贝尔曼期望方程的关系。

---

## 核心定义

### 问题陈述

**给定**：MDP \((S, A, P, R, \gamma)\) 和策略 \(\pi\)

**求解**：所有 \(s \in S\) 的 \(V^\pi(s)\)

### 迭代更新规则

$$V_{k+1}(s) = \sum_a \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]$$

**含义**：使用后继状态的当前估计值来更新每个状态的价值。

### 收敛判据

当满足以下条件时停止：

$$\max_s |V_{k+1}(s) - V_k(s)| < \theta$$

其中 \(\theta\) 是一个小阈值（例如 \(10^{-4}\)）。

---

## 数学与推导

### 为什么迭代有效：压缩映射

贝尔曼算子 \(\mathcal{T}^\pi\) 是一个**压缩映射**：

$$\|\mathcal{T}^\pi V_1 - \mathcal{T}^\pi V_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty$$

**含义**：应用贝尔曼更新会使任意两个价值函数以因子 \(\gamma\) 靠近。

**结果**：根据Banach不动点定理，迭代收敛到唯一不动点 \(V^\pi\)。

### 收敛速率

经过 \(k\) 次迭代后：

$$\|V_k - V^\pi\|_\infty \leq \gamma^k \|V_0 - V^\pi\|_\infty$$

**例子**：当 \(\gamma = 0.9\) 时，100次迭代后，误差减少了 \(0.9^{100} \approx 0.00003\) 倍。

### 矩阵形式解

贝尔曼方程是线性的：

$$\mathbf{V}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi$$

直接解：

$$\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{R}^\pi$$

**复杂度**：矩阵求逆是 \(O(|S|^3)\) — 只对小状态空间实用。

---

## 算法概述

```
算法：迭代策略评估

输入：MDP，策略 π，阈值 θ
输出：V^π

1. 初始化 V(s) = 0 对所有 s（或任意值）
2. 重复：
     Δ = 0
     对每个 s ∈ S：
         v = V(s)
         V(s) = Σ_a π(a|s) [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]
         Δ = max(Δ, |v - V(s)|)
   直到 Δ < θ
3. 返回 V
```

**原地更新 vs 双数组**：原地更新（立即覆盖 \(V(s)\)）通常收敛更快，因为使用了更新鲜的估计值。Sutton & Barto 使用原地更新。

### 复杂度

- **每次迭代**：\(O(|S|^2 |A|)\) — 对每个状态，对动作和下一个状态求和
- **迭代次数**：大约 \(O(\frac{1}{1-\gamma} \log \frac{1}{\theta})\)

---

## 常见陷阱

1. **忘记处理终止状态**：终止状态应该有 \(V(s_{terminal}) = 0\)（没有未来奖励）。

2. **错误的更新顺序**：对于原地更新，顺序影响速度但不影响正确性。随机或优先顺序可能有帮助。

3. **收敛检查不当**：检查最大绝对变化，而不是总和或平均值。

4. **初始化值不当**：零初始化是安全的。随机初始化可能更快，但如果值极端可能会导致问题。

---

## 小例子

**3状态链MDP：**

```
S0 ---(a, r=0)---> S1 ---(a, r=0)---> S2 (终止, r=1)
```

策略：只有一个可用动作（确定性的）。
\(\gamma = 0.9\)

**迭代：**
- \(V_0 = [0, 0, 0]\)
- \(V_1\)：\(V(S2) = 0\)，\(V(S1) = 0 + 0.9 \times 0 = 0\)，\(V(S0) = 0 + 0.9 \times 0 = 0\)

等等——我们到达终止状态时获得奖励1！让我们修正：
- \(V(S1) = 0 + 0.9 \times 1 = 0.9\)（到S2的转移时获得奖励）
- 实际上，最后一步的奖励：\(V(S1) = R(S1 \to S2) + \gamma \cdot 0 = 1\)
- \(V(S0) = R(S0 \to S1) + \gamma V(S1) = 0 + 0.9 \times 1 = 0.9\)

**最终结果**：\(V^\pi = [0.9, 1, 0]\)

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 为什么策略评估总是收敛？</summary>

**答案**：贝尔曼算子是一个压缩因子为 \(\gamma < 1\) 的压缩映射，所以根据Banach不动点定理，迭代收敛到唯一不动点。

**解释**：每次应用贝尔曼更新都会使任何价值估计更接近真实价值。压缩性质 \(\|\mathcal{T}^\pi V_1 - \mathcal{T}^\pi V_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty\) 保证了这一点。

**关键方程**：\(\|V_k - V^\pi\|_\infty \leq \gamma^k \|V_0 - V^\pi\|_\infty\)

**常见陷阱**：收敛只在 \(\gamma < 1\) 时保证。当 \(\gamma = 1\) 时，系统可能不收缩。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 原地更新和双数组更新有什么区别？</summary>

**答案**：
- **双数组**：分别存储 \(V_k\) 和 \(V_{k+1}\)；所有更新使用旧值
- **原地**：立即更新 \(V(s)\)；后面的状态使用更新后的值

**解释**：两者都收敛到相同的答案。原地更新通常收敛更快，因为它使用更新鲜的估计值。状态更新的顺序影响原地更新的速度（不影响正确性）。

**常见陷阱**：认为原地更新是错误的。实际上它是Sutton & Barto的标准方法。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 写出贝尔曼期望方程的矩阵形式及其解。</summary>

**答案**：

$$\mathbf{V}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi$$

$$\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{R}^\pi$$

**解释**：
- \(\mathbf{V}^\pi\) 是状态价值向量
- \(\mathbf{R}^\pi\) 是策略 \(\pi\) 下的期望即时奖励向量
- \(\mathbf{P}^\pi\) 是策略 \(\pi\) 下的转移矩阵：\(P^\pi_{ss'} = \sum_a \pi(a|s) P(s'|s,a)\)
- 重排：\((\mathbf{I} - \gamma \mathbf{P}^\pi) \mathbf{V}^\pi = \mathbf{R}^\pi\)

**常见陷阱**：矩阵求逆是 \(O(|S|^3)\) — 对大状态空间不实用。改用迭代方法。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> γ=0.99时，需要多少次迭代才能将误差减少1000倍？</summary>

**答案**：大约690次迭代。

**解释**：我们需要 \(\gamma^k < 0.001\)，所以 \(k > \frac{\log(0.001)}{\log(0.99)} = \frac{-6.9}{-0.01} \approx 690\)。

**关键洞见**：更大的 \(\gamma\) 意味着更慢的收敛。这是直观的：远视的智能体需要更多迭代来传播价值信息。

**常见陷阱**：低估高 \(\gamma\) 所需的迭代次数。实践中，使用 \(\gamma = 0.99\) 但接受近似收敛。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的策略评估运行非常慢。如何加速？</summary>

**答案**：策略：
1. **优先扫描**：首先更新贝尔曼误差最大的状态
2. **异步更新**：不必每次迭代都遍历所有状态
3. **降低精度阈值**：接受 \(\theta = 10^{-2}\) 而不是 \(10^{-6}\)
4. **降低 \(\gamma\)**：更快收敛（如果任务允许）
5. **Gauss-Seidel**（原地）：通常比Jacobi（双数组）更快

**解释**：瓶颈通常是迭代次数。优先扫描可以通过关注价值变化最大的状态来显著减少迭代次数。

**常见陷阱**：使用过于严格的收敛阈值。对于策略迭代，我们通常不需要精确的 \(V^\pi\) — 近似评估就足够了。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第4.1章
- **Bertsekas**, 动态规划与最优控制，第2卷
- **Puterman**, 马尔可夫决策过程，第6章

**面试需要记忆的**：更新规则，压缩性质，收敛速率 \(\gamma^k\)，矩阵形式（但注意不实用性），原地 vs 双数组。

**代码示例**：[policy_evaluation.py](../../../rl_examples/algorithms/policy_evaluation.py)
