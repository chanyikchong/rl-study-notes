# 策略迭代

## 面试摘要

**策略迭代** 在策略评估（计算 \(V^\pi\)）和策略改进（使策略相对于 \(V^\pi\) 贪婪）之间交替进行。它在有限次迭代内收敛到最优策略。每次改进步骤保证不会变差。这是理解Actor-Critic方法的基础。

**需要记忆的**：两步结构（评估、改进），策略改进定理，保证单调改进，有限收敛。

---

## 核心定义

### 算法结构

1. **策略评估**：计算当前策略 \(\pi\) 的 \(V^\pi\)
2. **策略改进**：创建相对于 \(V^\pi\) 贪婪的新策略 \(\pi'\)
3. 重复直到策略不再变化

### 策略改进步骤

$$\pi'(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]$$

或等价地：

$$\pi'(s) = \arg\max_a Q^\pi(s, a)$$

**含义**：对于每个状态，选择根据当前价值估计看起来最好的动作。

---

## 数学与推导

### 策略改进定理

**定理**：对于任何策略 \(\pi\)，如果我们定义：

$$\pi'(s) = \arg\max_a Q^\pi(s, a)$$

那么：

$$V^{\pi'}(s) \geq V^\pi(s) \quad \forall s$$

**证明概要**：

$$V^\pi(s) \leq Q^\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t=s, A_t=\pi'(s)]$$

$$\leq \mathbb{E}[R_{t+1} + \gamma Q^\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s, A_t=\pi'(s)]$$

$$\leq \ldots \leq V^{\pi'}(s)$$

通过重复应用，新策略在所有地方至少一样好。

### 收敛到最优

**关键洞见**：在有限MDP中，确定性策略的数量是有限的（\(|A|^{|S|}\)）。每次迭代严格改进或保持不变。由于我们不能永远改进，必须收敛到最优。

### 何时停止改进？

当满足以下条件时改进停止：

$$V^\pi(s) = \max_a Q^\pi(s, a) \quad \forall s$$

这正是贝尔曼最优方程！所以 \(V^\pi = V^*\) 且 \(\pi = \pi^*\)。

---

## 算法概述

```
算法：策略迭代

输入：MDP (S, A, P, R, γ)
输出：最优策略 π*，最优价值 V*

1. 对所有 s 任意初始化 π(s)
2. 重复：
     # 策略评估
     使用迭代策略评估计算 V^π

     # 策略改进
     policy_stable = True
     对每个 s ∈ S：
         old_action = π(s)
         π(s) = argmax_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]
         如果 old_action ≠ π(s)：
             policy_stable = False
   直到 policy_stable
3. 返回 π, V
```

### 复杂度

- **每次评估**：每次迭代 \(O(|S|^2|A|)\)，\(O(\frac{1}{1-\gamma})\) 次迭代
- **策略迭代次数**：最多 \(|A|^{|S|}\)，但实践中通常是 \(O(|S||A|)\)
- **总体**：实践中通常比价值迭代快得多

---

## 常见陷阱

1. **完全评估 vs 部分评估**：你不需要精确收敛 \(V^\pi\)。即使一次策略评估扫描也可以工作（修改的策略迭代）。

2. **任意打破平局**：当多个动作具有相同的Q值时，任何选择都可以。但要保持一致以检测收敛。

3. **随机策略**：标准策略迭代产生确定性策略。对于随机策略，需要软改进。

4. **与价值迭代混淆**：策略迭代完全评估然后改进。价值迭代将两者合并为一个max步骤。

---

## 小例子

**网格世界（2x2）：**

```
+---+---+
| A | G |   G = 目标，奖励 +1
+---+---+
| B | C |   除了到达G外，所有移动奖励为0
+---+---+
```

动作：上、下、左、右（确定性）。\(\gamma = 0.9\)。

**初始策略**：所有状态都向右走。

**迭代1 - 评估**：
- \(V(G) = 0\)（终止）
- \(V(A) = 0 + 0.9 \times 0 = 0\)... 等等，到达G给+1
- 实际上：\(V(A) = 1\)（A→G的奖励）
- \(V(B) = 0 + 0.9 \times V(C) = 0.9 V(C)\)
- \(V(C) = 0 + 0.9 \times 0 = 0\)（C→右→C... 卡住了！）

**迭代1 - 改进**：
- 在B处：比较上（\(V(A)=1\)）vs 右（\(V(C)=0\)）→ 切换到上
- 在C处：比较上（\(V(G)=0\) 但奖励1）vs 右（0）→ 切换到上

**改进后**：B→上，C→上，A→右（已经是最优）。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 为什么策略迭代保证收敛？</summary>

**答案**：确定性策略的数量是有限的。每次改进步骤产生一个至少一样好的策略（策略改进定理）。由于我们只能改进有限次，必须收敛。

**解释**：如果改进后 \(\pi' \neq \pi\)，那么 \(V^{\pi'} \geq V^\pi\)，至少有一个状态是严格不等式。由于只有 \(|A|^{|S|}\) 种可能的策略，我们不能永远改进。

**关键洞见**：收敛是有限的（不像价值迭代那样只是渐近的）。

**常见陷阱**：认为策略迭代可能会振荡。改进定理保证单调进展。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 策略迭代和Actor-Critic之间有什么关系？</summary>

**答案**：Actor-Critic本质上是在线策略迭代：
- **Critic** = 策略评估（估计 \(V^\pi\) 或 \(Q^\pi\)）
- **Actor** = 策略改进（更新策略朝向更好的动作）

**解释**：策略迭代先完全评估然后完全改进。Actor-Critic交替进行：稍微更新价值估计，稍微更新策略，重复。底层逻辑是相同的。

**常见陷阱**：认为Actor-Critic是根本不同的。它实际上只是带函数逼近的增量策略迭代。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 精确陈述策略改进定理。</summary>

**答案**：设 \(\pi\) 是任何策略，\(\pi'\) 是相对于 \(Q^\pi\) 贪婪的：

$$\pi'(s) = \arg\max_a Q^\pi(s, a)$$

那么：

$$V^{\pi'}(s) \geq V^\pi(s) \quad \forall s \in S$$

当且仅当 \(\pi\) 已经是最优时等号成立。

**解释**：相对于当前价值估计贪婪地行动永远不会使事情变坏。如果它不改变策略，我们就找到了最优。

**关键方程**：\(V^\pi(s) \leq Q^\pi(s, \pi'(s)) \leq V^{\pi'}(s)\)

**常见陷阱**：这假设我们正确计算了 \(Q^\pi\)。有了函数逼近，没有保证！
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 为什么你可能不需要精确的策略评估？</summary>

**答案**：即使近似的 \(V^\pi\) 也可以产生正确的贪婪策略。重要的是动作的相对排序，而不是绝对值。

**解释**：如果 \(Q^\pi(s,a_1) > Q^\pi(s,a_2)\)，\(V\) 中的小误差不会改变这个排序。所以我们可以使用**修改的策略迭代**：在改进前只做几次评估扫描。极端情况：每次改进一次扫描 = 价值迭代。

**关键洞见**：通过用更多改进步骤换取精确评估，整体收敛更快。

**常见陷阱**：在评估上投入过多。几次扫描通常就足够了。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的策略迭代不断在两个策略之间切换。出了什么问题？</summary>

**答案**：正确实现时这不应该发生。调试步骤：
1. 检查Q值平局时是否有不一致的打破平局
2. 验证评估收敛阈值不是太松
3. 检查浮点精度问题
4. 确保你正确比较策略（不是值）

**解释**：策略改进定理保证严格改进或达到最优。振荡表示有bug。最可能的是：打破平局不是确定性的，或者评估没有正确收敛。

**常见陷阱**：在argmax中使用 `>` vs `>=` 会导致平局问题。使用一致的规则，如"最小动作索引赢得平局"。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第4.2-4.3章
- **Howard (1960)**, 动态规划与马尔可夫过程 — 原始策略迭代
- **Bertsekas**, 动态规划与最优控制

**面试需要记忆的**：两步结构，策略改进定理陈述，为什么保证收敛，与Actor-Critic的联系。

**代码示例**：[policy_iteration.py](../../../rl_examples/algorithms/policy_iteration.py)
