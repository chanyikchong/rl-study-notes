# 价值迭代

## 面试摘要

**价值迭代** 通过迭代贝尔曼最优方程直接计算最优价值 \(V^*\)。与策略迭代不同，它在迭代过程中不维护显式策略——只是对动作取max。渐近收敛（不是有限步骤）。比策略迭代更容易实现；当 \(|A|\) 较小时通常是首选。

**需要记忆的**：带max的更新规则，与贝尔曼最优的关系，与策略迭代的区别，收敛保证。

---

## 核心定义

### 贝尔曼最优算子

$$(\mathcal{T}^* V)(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s') \right]$$

### 迭代更新

$$V_{k+1}(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]$$

**与策略评估的关键区别**：使用 \(\max\) 而不是对策略的期望。

### 提取策略

收敛后，提取最优策略：

$$\pi^*(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]$$

---

## 数学与推导

### 压缩性质

贝尔曼最优算子 \(\mathcal{T}^*\) 也是一个压缩映射：

$$\|\mathcal{T}^* V_1 - \mathcal{T}^* V_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty$$

**证明直觉**：max不破坏压缩性，因为：

$$|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|$$

### 收敛速率

与策略评估相同：

$$\|V_k - V^*\|_\infty \leq \gamma^k \|V_0 - V^*\|_\infty$$

### 价值迭代作为策略迭代

价值迭代可以看作是只做**一次**策略评估扫描的策略迭代：
- 从 \(\pi_k\) 开始（隐式：相对于 \(V_k\) 贪婪）
- 做一次评估扫描
- 立即改进到 \(\pi_{k+1}\)

这就是为什么价值迭代被称为"截断的策略迭代"。

### Q值形式

我们也可以直接在Q值上迭代：

$$Q_{k+1}(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q_k(s', a')$$

这是Q学习（基于采样的版本）的基础。

---

## 算法概述

```
算法：价值迭代

输入：MDP (S, A, P, R, γ)，阈值 θ
输出：最优策略 π*，近似最优 V*

1. 对所有 s 初始化 V(s) = 0
2. 重复：
     Δ = 0
     对每个 s ∈ S：
         v = V(s)
         V(s) = max_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]
         Δ = max(Δ, |v - V(s)|)
   直到 Δ < θ
3. 提取策略：
   对每个 s ∈ S：
     π(s) = argmax_a [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]
4. 返回 π, V
```

### 复杂度

- **每次迭代**：\(O(|S|^2|A|)\)
- **收敛迭代次数**：\(O(\frac{1}{1-\gamma} \log \frac{1}{\theta})\)
- 通常比策略迭代需要更多迭代，但每次迭代更便宜

---

## 常见陷阱

1. **迭代过程中没有显式策略**：不要尝试跟踪策略——只跟踪价值。最后提取策略。

2. **过早停止**：价值迭代渐近收敛。需要检查策略稳定性才能真正终止。

3. **与策略迭代混淆**：价值迭代使用max（一个方程）。策略迭代使用两个步骤（用期望评估，然后用max改进）。

4. **动作空间扩展**：每个状态需要对所有动作取max。大动作空间代价高。

---

## 小例子

**简单2状态MDP：**

状态：{A, B}。从A：动作1保持（r=2），动作2去B（r=0）。从B：只有一个动作保持（r=1）。\(\gamma = 0.5\)。

**迭代0**：\(V = [0, 0]\)

**迭代1**：
- \(V(A) = \max\{2 + 0.5 \times 0, \; 0 + 0.5 \times 0\} = 2\)
- \(V(B) = 1 + 0.5 \times 0 = 1\)
- \(V = [2, 1]\)

**迭代2**：
- \(V(A) = \max\{2 + 0.5 \times 2, \; 0 + 0.5 \times 1\} = \max\{3, 0.5\} = 3\)
- \(V(B) = 1 + 0.5 \times 1 = 1.5\)
- \(V = [3, 1.5]\)

**迭代3**：
- \(V(A) = \max\{2 + 0.5 \times 3, \; 0 + 0.5 \times 1.5\} = \max\{3.5, 0.75\} = 3.5\)
- \(V(B) = 1 + 0.5 \times 1.5 = 1.75\)

**收敛到**：\(V^*(A) = 4\)，\(V^*(B) = 2\)（几何级数：\(2 + 2 \times 0.5 + 2 \times 0.25 + \ldots = 4\)）

**最优策略**：永远待在A。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 价值迭代和策略迭代有什么关键区别？</summary>

**答案**：
- **价值迭代**：带max的一个方程；没有显式策略；渐近收敛
- **策略迭代**：两个步骤（评估、改进）；显式策略；有限迭代内收敛

**解释**：价值迭代通过使用max将评估和改进合并到一个更新中。策略迭代将它们分开。价值迭代更简单但收敛更慢。策略迭代收敛更快但每次迭代更昂贵。

**常见陷阱**：认为价值迭代因为更简单就一定更好。对于许多问题，策略迭代总操作数更少。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么价值迭代比策略迭代收敛更慢？</summary>

**答案**：价值迭代在改进前只做一次评估扫描，而策略迭代做完整评估。额外的评估帮助每次改进步骤更准确。

**解释**：可以把它想象成优化中的探索vs利用。策略迭代在改变方向前充分利用当前价值估计。价值迭代更贪婪——每次扫描都改变方向。

**关键洞见**：修改的策略迭代（几次评估扫描）通常能达到最佳平衡。

**常见陷阱**：假设更多迭代意味着更多计算。每次价值迭代步骤比完整策略评估便宜。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 证明贝尔曼最优算子是压缩映射。</summary>

**答案**：对于任意两个价值函数 \(V_1, V_2\)：

$$|(\mathcal{T}^* V_1)(s) - (\mathcal{T}^* V_2)(s)|$$

$$= |\max_a [R + \gamma \sum_{s'} P(s'|s,a) V_1(s')] - \max_a [R + \gamma \sum_{s'} P(s'|s,a) V_2(s')]|$$

$$\leq \max_a |\gamma \sum_{s'} P(s'|s,a) (V_1(s') - V_2(s'))|$$

$$\leq \gamma \max_a \sum_{s'} P(s'|s,a) |V_1(s') - V_2(s')|$$

$$\leq \gamma \|V_1 - V_2\|_\infty$$

**关键步骤**：\(|\max f - \max g| \leq \max |f - g|\)。

**常见陷阱**：max不破坏压缩性——这在直觉上经常令人困惑。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 何时价值迭代优于策略迭代？</summary>

**答案**：价值迭代在以下情况下更优：
1. 动作空间小（max便宜）
2. 我们只需要近似价值（可以提前停止）
3. 实现简单性重要
4. 状态空间相对动作空间大

**解释**：策略迭代的优势是更快收敛（更少外层迭代）。但每次迭代需要策略评估收敛。如果评估昂贵（大状态空间）且我们可以接受近似，价值迭代获胜。

**常见陷阱**：假设一个总是更好。选择取决于问题结构。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 价值迭代对你的MDP很慢。如何加速？</summary>

**答案**：策略：
1. **优先扫描**：首先更新贝尔曼误差最大的状态
2. **Gauss-Seidel（原地）**：立即使用更新的值
3. **异步更新**：关注相关状态
4. **降低 \(\gamma\)**：更快收敛（如果可接受）
5. **提前终止**：在完全收敛前提取策略
6. **使用策略迭代**：对这个问题可能更快

**解释**：优先扫描可以将迭代次数减少10-100倍。关键洞见：更新从目标状态向后传播。优先处理后继刚刚改变的状态。

**常见陷阱**：不考虑切换到策略迭代。有时"更简单"的算法实际上更慢。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第4.4章
- **Bellman (1957)**, 动态规划
- **Bertsekas**, 动态规划与最优控制，第1卷

**面试需要记忆的**：带max的更新规则，压缩性质（相同的 \(\gamma^k\) 速率），与策略迭代的区别，何时优先选择哪个。

**代码示例**：[value_iteration.py](../../../rl_examples/algorithms/value_iteration.py)
