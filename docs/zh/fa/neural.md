# 神经网络函数逼近

## 面试摘要

**神经网络函数逼近** 使用深度网络表示价值函数：\(\hat{V}(s; \boldsymbol{\theta})\) 或 \(\hat{Q}(s, a; \boldsymbol{\theta})\)。关键优势：自动学习特征（无需手工工程）。关键挑战：没有收敛保证，不稳定（致命三角）。现代深度RL（DQN、A3C、PPO）都使用神经FA。理解稳定性问题对面试至关重要。

**需要记忆的**：神经TD更新，致命三角，DQN技巧（回放、目标网络），为什么不稳定发生。

---

## 核心定义

### 神经价值函数

$$\hat{V}(s; \boldsymbol{\theta}) = f_\theta(s)$$

其中 \(f_\theta\) 是参数为 \(\boldsymbol{\theta}\) 的神经网络。

### 神经Q函数

$$\hat{Q}(s, a; \boldsymbol{\theta}) = f_\theta(s, a) \quad \text{或} \quad \hat{Q}(s, \cdot; \boldsymbol{\theta}) = f_\theta(s)$$

第二种形式输出所有动作的Q值（对离散动作更高效）。

### 梯度

$$\nabla_\theta \hat{V}(s; \boldsymbol{\theta}) = \nabla_\theta f_\theta(s)$$

通过反向传播计算。与线性FA不同，梯度是 \(\boldsymbol{\theta}\) 的复杂函数。

---

## 数学与推导

### 半梯度TD更新

$$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \alpha \delta_t \nabla_\theta \hat{V}(S_t; \boldsymbol{\theta})$$

其中 \(\delta_t = R_{t+1} + \gamma \hat{V}(S_{t+1}; \boldsymbol{\theta}) - \hat{V}(S_t; \boldsymbol{\theta})\)。

**与线性FA相同**，但 \(\nabla_\theta\) 现在是神经网络梯度。

### 为什么神经+TD可能发散

**致命三角**：
1. **函数逼近**：一个状态的更新影响其他状态
2. **自举**：目标依赖当前估计
3. **异策略**：数据分布 ≠ 目标分布

这三者一起可以导致价值螺旋到无穷大。

### 损失函数视角

DQN最小化：

$$L(\boldsymbol{\theta}) = \mathbb{E}_{(s,a,r,s') \sim D}\left[ (y - \hat{Q}(s, a; \boldsymbol{\theta}))^2 \right]$$

其中目标 \(y = r + \gamma \max_{a'} \hat{Q}(s', a'; \boldsymbol{\theta}^-)\)。

**注意**：目标网络 \(\boldsymbol{\theta}^-\) 是冻结的——这稳定了训练。

---

## 算法概述

### 朴素神经TD（不稳定）

```
算法：神经TD（不要直接使用）

1. 随机初始化 θ
2. 对每一步：
     观察 (S, A, R, S')
     δ = R + γ V(S'; θ) - V(S; θ)
     θ ← θ + α · δ · ∇_θ V(S; θ)
```

**问题**：相关更新，移动目标 → 发散。

### DQN风格（稳定）

```
算法：DQN（稳定的神经Q学习）

1. 初始化 θ, θ^- = θ, 回放缓冲区 D
2. 对每一步：
     A = 从 Q(S, ·; θ) 的 ε-贪婪
     执行 A，观察 R, S'
     存储 (S, A, R, S') 到 D

     从 D 采样小批量 {(s, a, r, s')}
     计算目标：y = r + γ max_a' Q(s', a'; θ^-)
     损失：L = Σ(y - Q(s, a; θ))²
     θ ← θ - α∇_θ L

     每 C 步：θ^- ← θ
```

**关键添加**：回放缓冲区（打破相关性），目标网络（稳定目标）。

---

## 常见陷阱

1. **没有回放缓冲区**：顺序数据是相关的 → 有偏梯度 → 发散。

2. **没有目标网络**：追逐移动目标 → 振荡/发散。

3. **学习率太高**：神经网络需要小学习率（1e-4到1e-3）。

4. **奖励缩放**：大奖励 → 大梯度 → 不稳定。裁剪或归一化奖励。

5. **网络太大/太小**：过参数化可能有帮助但代价高。欠参数化限制表示。

6. **忽视双Q学习**：标准Q学习过估计 → 使用Double DQN。

---

## 小例子

**带神经Q的CartPole：**

网络：4输入（状态）→ 128隐藏 → 2输出（左/右的Q）

训练：
1. 用ε-贪婪收集经验
2. 存储在回放缓冲区（大小10000）
3. 采样32个转移的批量
4. 使用目标网络计算TD目标
5. 对MSE损失做梯度步
6. 每100步更新目标网络

**结果**：在约200个回合内解决CartPole。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 什么是"致命三角"，为什么它导致问题？</summary>

**答案**：致命三角由以下组成：
1. 函数逼近
2. 自举
3. 异策略学习

**解释**：函数逼近意味着更新一个状态影响其他状态。自举使用当前估计作为目标（自指涉）。异策略意味着数据分布与我们学习的不匹配。一起：更新可以放大误差并失控。

**关键洞见**：每个元素单独是没问题的。没有FA的TD收敛。带MC的FA收敛。是组合才危险。

**常见陷阱**：认为任何一个元素是罪魁祸首。三个都需要才会发散。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么经验回放帮助稳定学习？</summary>

**答案**：它打破连续样本之间的相关性。

**解释**：没有回放，连续转移高度相关（来自同一轨迹）。这违反了SGD的i.i.d.假设，导致有偏梯度。回放打乱经验，使小批量更接近i.i.d.。

**额外好处**：数据效率——每个转移使用多次。

**常见陷阱**：认为回放只是为了效率。去相关性更重要。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 解释目标网络如何工作以及为什么它们有帮助。</summary>

**答案**：目标网络 \(\theta^-\) 是主网络 \(\theta\) 的周期性副本。目标使用 \(\theta^-\)：

$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

**解释**：没有目标网络，目标每次梯度步都变化（追逐移动目标）。通过冻结 \(\theta^-\)，目标在C步内稳定，允许梯度下降取得进展。

**关键洞见**：我们在做带固定目标的回归（像监督学习），这是稳定的。

**常见陷阱**：过于频繁更新 \(\theta^-\) 会抵消好处。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 神经Q学习中的过估计偏差是什么？</summary>

**答案**：对带噪声Q估计取max倾向于过估计。

$$\mathbb{E}[\max_a Q(s, a)] \geq \max_a \mathbb{E}[Q(s, a)]$$

**解释**：神经网络有估计误差。max选择哪个动作估计Q最高——这通常是过估计。这通过自举累积。

**解决方案**：Double DQN：用一个网络选择动作，另一个评估：

$$y = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-)$$

**常见陷阱**：实践中忽视这个。Double DQN通常更好。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的DQN没有学习。调试什么？</summary>

**答案**：调试清单：
1. **奖励可见**：打印奖励验证环境工作
2. **探索充分**：ε够高吗？衰减太快？
3. **学习率**：尝试1e-4到1e-3
4. **回放缓冲区大小**：至少10K，最好100K+
5. **目标更新频率**：每1000-10000步
6. **网络架构**：先尝试简单网络
7. **梯度裁剪**：裁剪到[-1, 1]或使用Huber损失
8. **奖励裁剪/归一化**：保持奖励在[-1, 1]

**解释**：DQN有很多超参数。从论文中已知有效的设置开始，然后调整。

**常见陷阱**：同时改变很多东西调试。一次改一个东西。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第11章
- **Mnih et al. (2015)**, 通过深度RL实现人类水平控制（DQN）
- **Van Hasselt et al. (2016)**, 带双Q学习的深度强化学习
- **Tsitsiklis & Van Roy (1997)**, 带函数逼近的TD(λ)分析

**面试需要记忆的**：致命三角，回放缓冲区目的，目标网络目的，Double DQN，过估计偏差。

**代码示例**：[dqn.py](../../../rl_examples/algorithms/dqn.py)
