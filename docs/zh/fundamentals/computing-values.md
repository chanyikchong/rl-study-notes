# 实际计算价值函数

## 面试摘要

计算价值函数看起来难以处理——它们被定义为对无限多未来轨迹的期望。在实践中，RL使用三种方法：**动态规划**（基于模型，迭代贝尔曼更新）、**蒙特卡洛**（采样完整回合，平均回报）和**时序差分**（使用当前估计进行自举）。对于大状态空间，**函数逼近**在状态间进行泛化。关键洞见：贝尔曼方程的递归结构将无限时域问题转化为可处理的单步更新。

**需要记忆的**：为什么朴素计算不可行，DP/MC/TD方法，自举概念，MC和TD之间的偏差-方差权衡。

---

## 设计动机：问题所在

### 难以处理的定义

价值函数被定义为对所有未来轨迹的期望：

$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\right]$$

**问题**：这个期望是对以下所有情况求和：
- 所有可能的动作序列（指数级多）
- 所有可能的状态转移（随机环境）
- 无限时间范围（对于持续任务）

**朴素方法**：枚举所有轨迹，按概率加权，求加权回报之和。

**为什么失败**：
- 轨迹数量：$|A|^T \times$（随机转移的分支）
- 对于 $|A|=4$，$T=100$：$4^{100} \approx 10^{60}$ 条轨迹
- 计算上不可能！

### 关键洞见：贝尔曼方程

贝尔曼方程提供了解决方案：

$$V(s) = \mathbb{E}\left[ r + \gamma V(s') \right]$$

**为什么有帮助**：
- 将无限时域期望转换为**单步**期望
- 未来价值由 $V(s')$ 概括
- 我们只需要向前看一步！

这种递归结构使RL变得可行。

---

## 核心定义

### 三种方法

| 方法 | 需要 | 更新时机 | 偏差 | 方差 |
|----------|----------|---------|------|----------|
| **动态规划** | 模型（$P$, $R$） | 对 $s'$ 完整期望 | 无 | 无 |
| **蒙特卡洛** | 完整回合 | 回合结束后 | 无 | 高 |
| **时序差分** | 单个转移 | 每步 | 有（如果 $V$ 错误） | 低 |

### "自举"的含义

**自举** = 使用你自己的估计来更新你的估计。

- **不自举**（MC）：使用实际观察到的回报 $G_t$
- **自举**（TD, DP）：使用估计的未来价值 $V(s')$

```
MC:  V(s) ← 实际回报 G 的平均
TD:  V(s) ← V(s) + α[r + γV(s') - V(s)]
                            ↑
                    这就是自举！
```

---

## 三种方法详解

### 方法1：动态规划（基于模型）

**需求**：完整MDP模型——转移概率 $P(s'|s,a)$ 和奖励 $R(s,a)$。

**方法**：迭代应用贝尔曼方程：

$$V_{k+1}(s) = \sum_a \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \right]$$

**工作原理**：

```
初始化 V(s) = 0 对所有状态

重复直到收敛：
    对每个状态 s：
        V_new(s) = Σ_a π(a|s) × [R(s,a) + γ × Σ_s' P(s'|s,a) × V(s')]
    V = V_new

返回 V
```

**为什么收敛**：贝尔曼算子是压缩映射。每次迭代使 $V$ 更接近真实值。对于 $\gamma < 1$ 保证收敛。

**复杂度**：每次迭代 $O(|S|^2 |A|)$——仅对小状态空间可行。

**例子**：

```
两状态MDP，策略 π(停留) = 1：
- 状态 A：R(A) = 2，停在 A
- 状态 B：R(B) = 1，停在 B
- γ = 0.9

迭代 0：V(A) = 0, V(B) = 0
迭代 1：V(A) = 2 + 0.9×0 = 2
        V(B) = 1 + 0.9×0 = 1
迭代 2：V(A) = 2 + 0.9×2 = 3.8
        V(B) = 1 + 0.9×1 = 1.9
...
收敛到：V(A) = 2/(1-0.9) = 20
        V(B) = 1/(1-0.9) = 10
```

---

### 方法2：蒙特卡洛（无模型，基于采样）

**需求**：能够运行回合（不需要模型）。

**方法**：用样本平均替代期望：

$$V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_i(s)$$

其中 $G_i(s)$ 是在第 $i$ 个回合从状态 $s$ 开始观察到的回报。

**工作原理**：

```
初始化 V(s) = 0, counts(s) = 0 对所有状态

对每个回合：
    生成轨迹：s₀, a₀, r₁, s₁, a₁, r₂, ..., s_T

    对轨迹中的每个状态 s_t：
        G_t = r_{t+1} + γr_{t+2} + γ²r_{t+3} + ... + γ^(T-t)r_T
        counts(s_t) += 1
        V(s_t) += (G_t - V(s_t)) / counts(s_t)   # 滑动平均

返回 V
```

**为什么有效**：大数定律——样本平均收敛到真实期望。

**优点**：
- 不需要模型
- 无偏差（使用实际回报）
- 适用于非马尔可夫问题

**缺点**：
- 高方差（每个回报是许多随机变量的和）
- 必须等到回合结束
- 仅适用于回合制任务

**例子**：

```
网格世界：从 A 开始，目标在 G（奖励 +10），γ = 0.9

回合 1：A → B → C → G（奖励：0, 0, 10）
        G(A) = 0 + 0.9×0 + 0.81×10 = 8.1

回合 2：A → B → G（奖励：0, 10）
        G(A) = 0 + 0.9×10 = 9.0

回合 3：A → C → B → G（奖励：0, 0, 10）
        G(A) = 0 + 0.9×0 + 0.81×10 = 8.1

V(A) ≈ (8.1 + 9.0 + 8.1) / 3 = 8.4
```

---

### 方法3：时序差分（无模型，自举）

**需求**：单个转移（不需要完整回合）。

**方法**：向单步自举目标更新：

$$V(s) \leftarrow V(s) + \alpha \left[ r + \gamma V(s') - V(s) \right]$$

**关键思想**：使用当前估计 $V(s')$ 作为所有未来回报的替代。

**工作原理**：

```
任意初始化 V(s)

对环境中的每一步：
    观察当前状态 s
    采取动作 a，观察奖励 r，下一状态 s'

    # TD 更新
    TD_target = r + γ × V(s')
    TD_error = TD_target - V(s)
    V(s) = V(s) + α × TD_error

    s = s'
```

**为什么有效**（直觉）：
- 如果 $V(s')$ 是正确的，那么 $r + \gamma V(s')$ 是 $V(s)$ 的无偏样本
- $V(s')$ 中的错误会在我们更新所有状态时得到纠正
- 随着时间推移，整个系统变得自洽

**优点**：
- 每步更新（无需等待回合结束）
- 比MC方差更低
- 适用于持续（非回合制）任务

**缺点**：
- 如果 $V$ 估计错误则有偏差（随着 $V$ 改善而收敛）
- 需要更仔细的学习率调整

**例子**：

```
同样的网格世界，α = 0.1，γ = 0.9

初始：V(A) = 0, V(B) = 0, V(C) = 0, V(G) = 0

步骤 1：A → B（r = 0）
        TD_target = 0 + 0.9 × V(B) = 0 + 0.9 × 0 = 0
        V(A) = 0 + 0.1 × (0 - 0) = 0

步骤 2：B → G（r = 10）
        TD_target = 10 + 0.9 × V(G) = 10 + 0.9 × 0 = 10
        V(B) = 0 + 0.1 × (10 - 0) = 1.0

步骤 3：A → B（r = 0）[新回合]
        TD_target = 0 + 0.9 × V(B) = 0 + 0.9 × 1.0 = 0.9
        V(A) = 0 + 0.1 × (0.9 - 0) = 0.09

# 价值通过状态向后传播！
```

---

### 方法4：函数逼近（扩展规模）

**问题**：当 $|S|$ 很大或连续时，无法为每个状态存储 $V(s)$。

**解决方案**：学习参数化函数 $V_\theta(s)$ 进行泛化：

$$V_\theta(s) \approx V^\pi(s)$$

**方法**：对TD误差进行梯度下降：

$$\theta \leftarrow \theta + \alpha \left[ r + \gamma V_\theta(s') - V_\theta(s) \right] \nabla_\theta V_\theta(s)$$

**工作原理**：

```
初始化神经网络 V_θ

对每个转移 (s, a, r, s')：
    # 计算 TD 目标（无梯度流过此处！）
    target = r + γ × V_θ(s').detach()

    # 计算损失
    loss = (target - V_θ(s))²

    # 梯度步骤
    θ = θ - α × ∇_θ loss
```

**为什么有效**：
- 神经网络学习模式："相似状态有相似价值"
- 泛化到未见过的状态
- 能够在高维空间（图像、连续状态）中学习

**挑战**：
- 无收敛保证（致命三元组）
- 需要仔细的稳定化（目标网络等）

---

## 比较：MC vs TD

| 方面 | 蒙特卡洛 | TD学习 |
|--------|-------------|-------------|
| **更新时机** | 回合结束后 | 每步 |
| **偏差** | 无偏 | 有偏（初期） |
| **方差** | 高 | 低 |
| **自举** | 否 | 是 |
| **持续任务** | 否 | 是 |
| **初始估计重要吗** | 否 | 是 |

### 偏差-方差权衡

**蒙特卡洛**：
- 使用实际回报 $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...$
- 许多随机变量之和 → 高方差
- 但它是真实回报 → 无偏差

**TD学习**：
- 使用 $r_t + \gamma V(s_{t+1})$——只有一个随机奖励
- 方差低得多
- 但如果 $V(s_{t+1})$ 错误 → 有偏差
- 偏差随 $V$ 改善而减少

**在实践中**：TD通常获胜，因为方差减少比消除小偏差更有价值。

---

## 深层洞见

贝尔曼方程是使RL可行的关键：

$$V(s) = \mathbb{E}\left[ r + \gamma V(s') \right]$$

**这种递归结构意味着**：
- 我们不需要无限地模拟未来
- 我们只需要**向前看一步**
- 未来价值由我们当前的估计 $V(s')$ 概括
- 迭代更新逐渐传播正确的值

**没有贝尔曼方程**，RL将需要完整的轨迹枚举——对于任何实际问题都是计算上不可能的。

---

## 算法概述

### 价值计算的统一视角

```
V(s) 的目标：
├── DP：   Σ_a π(a|s) [R(s,a) + γ Σ_s' P(s'|s,a) V(s')]  （完整期望）
├── MC：   G_t = r_t + γr_{t+1} + γ²r_{t+2} + ...         （采样回报）
└── TD(0)：r_t + γV(s_{t+1})                              （自举）

更新规则：
V(s) ← V(s) + α × [目标 - V(s)]
```

### TD(n)：连接MC和TD

我们可以使用n步回报进行插值：

$$G_t^{(n)} = r_t + \gamma r_{t+1} + ... + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})$$

| n | 方法 | 偏差 | 方差 |
|---|--------|------|----------|
| 1 | TD(0) | 高 | 低 |
| 2-5 | TD(n) | 中 | 中 |
| ∞ | MC | 无 | 高 |

---

## 常见陷阱

1. **以为我们枚举轨迹**：我们从不显式计算完整期望——我们使用样本或迭代更新。

2. **忘记自举引入偏差**：当价值估计错误时，TD估计是有偏的。这通常是可接受的，因为方差减少更重要。

3. **对持续任务使用MC**：蒙特卡洛要求回合结束。对于持续任务，使用TD。

4. **忽略DP的模型要求**：动态规划需要知道 $P(s'|s,a)$ 和 $R(s,a)$。没有模型时，使用MC或TD。

5. **不理解为什么TD有效**：TD"自己拉着自己的靴带上升"——估计相互改进。这看起来是循环的，但由于压缩性质而收敛。

6. **期望快速收敛**：价值传播需要许多迭代。在100个状态的链中，信息必须传播通过所有状态。

---

## 小例子：比较三种方法

**设置**：5个状态的线性链MDP，目标在状态5。

```
[1] → [2] → [3] → [4] → [5=目标]
         目标处奖励 = 1，其他地方 = 0
         γ = 0.9
```

真实值：$V(1) = 0.9^4 = 0.66$，$V(2) = 0.9^3 = 0.73$，$V(3) = 0.9^2 = 0.81$，$V(4) = 0.9$

### 动态规划

```
迭代 0：V = [0, 0, 0, 0, 0]
迭代 1：V = [0, 0, 0, 0.9, 1.0]  # 目标值 = 1，状态4得到 0.9×1
迭代 2：V = [0, 0, 0.81, 0.9, 1.0]  # 值向后传播
迭代 3：V = [0, 0.73, 0.81, 0.9, 1.0]
迭代 4：V = [0.66, 0.73, 0.81, 0.9, 1.0]  # 收敛！
```

**4次迭代传播通过4个状态。**

### 蒙特卡洛

```
回合 1：1→2→3→4→5（r=1）
  G(1) = 0.9⁴×1 = 0.66
  G(2) = 0.9³×1 = 0.73
  ...

回合 2：1→2→3→4→5（r=1）
  相同回报（确定性环境）

V(1) ≈ 0.66 仅在1个回合后！
```

**确定性环境立即收敛**（但如果随机则高方差）。

### TD学习

```
α = 0.1，初始 V = [0, 0, 0, 0, 0]

步骤 1→2：V(1) += 0.1×(0 + 0.9×0 - 0) = 0
步骤 2→3：V(2) += 0.1×(0 + 0.9×0 - 0) = 0
步骤 3→4：V(3) += 0.1×(0 + 0.9×0 - 0) = 0
步骤 4→5：V(4) += 0.1×(1 + 0.9×0 - 0) = 0.1  # 第一次学习！
步骤 5=目标：回合结束

# 下一个回合
步骤 1→2：V(1) += 0.1×(0 + 0.9×0 - 0) = 0
...
步骤 3→4：V(3) += 0.1×(0 + 0.9×0.1 - 0) = 0.009  # 值传播！
步骤 4→5：V(4) += 0.1×(1 + 0 - 0.1) = 0.19

# 值通过许多回合缓慢向后传播
```

**对于这个简单情况比MC慢，但对于随机环境方差更低。**

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 为什么我们不能通过枚举所有可能的轨迹来计算V(s)？</summary>

**答案**：轨迹数量是指数级大的，通常是无限的。

**解释**：对于 $|A|$ 个动作和 $T$ 个时间步，有 $O(|A|^T)$ 个可能的动作序列。随着随机转移，每个动作导致多个可能的下一状态，进一步乘以路径数。对于持续任务（$T = \infty$），有无限多条轨迹。

**例子**：4个动作和100个时间步：$4^{100} \approx 10^{60}$ 条轨迹——比宇宙中的原子还多！

**关键洞见**：贝尔曼方程通过使用递归结构避免了这个问题：$V(s) = E[r + \gamma V(s')]$。

**常见陷阱**：认为RL实际上显式计算期望。它使用样本（MC，TD）或迭代更新（DP）。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> "自举"是什么意思，哪些方法使用它？</summary>

**答案**：自举意味着使用你自己的估计来更新你的估计。

**解释**：
- **TD和DP自举**：使用 $V(s')$ 更新 $V(s)$，而 $V(s')$ 本身是估计
- **MC不自举**：使用实际采样的回报 $G_t$

**公式比较**：
- TD：$V(s) \leftarrow V(s) + \alpha[r + \gamma V(s') - V(s)]$——使用 $V(s')$
- MC：$V(s) \leftarrow V(s) + \alpha[G_t - V(s)]$——使用实际回报

**权衡**：自举引入偏差（如果估计错误）但减少方差（更少随机变量）。

**常见陷阱**：认为自举是循环的不应该有效。由于贝尔曼算子是压缩映射，它会收敛。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 解释MC和TD之间的偏差-方差权衡。</summary>

**答案**：MC无偏但高方差；TD低方差但有初始偏差。

**解释**：

**蒙特卡洛**：
- 目标：$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...$
- 这是真实回报 → $V(s)$ 的无偏估计
- 许多随机变量之和 → 高方差

**TD学习**：
- 目标：$r_t + \gamma V(s_{t+1})$
- 只有一个随机奖励 → 低方差
- 如果 $V(s_{t+1})$ 错误 → 有偏差
- 偏差随学习中 $V$ 改善而减少

**关键方程**：$\text{Var}(G_t) = \text{Var}(\sum_k \gamma^k r_k) \gg \text{Var}(r_t + \gamma V(s'))$

**常见陷阱**：认为偏差总是不好的。在实践中，TD的方差减少通常超过其偏差。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 为什么值迭代收敛？什么是压缩性质？</summary>

**答案**：贝尔曼算子是压缩映射——它在每次迭代时使价值估计更接近。

**解释**：对于任意两个价值函数 $V_1, V_2$：

$$\|T V_1 - T V_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty$$

其中 $T$ 是贝尔曼算子。由于 $\gamma < 1$，任意两个价值函数之间的距离每次迭代收缩 $\gamma$ 倍。

**结果**：
- $k$ 次迭代后，误差最多是 $\gamma^k \times$ 初始误差
- 对于 $\gamma = 0.99$，100次迭代后：$0.99^{100} \approx 0.37$
- 对于 $\gamma = 0.9$，100次迭代后：$0.9^{100} \approx 10^{-5}$

**常见陷阱**：对持续任务使用 $\gamma = 1$。无压缩 → 无收敛保证！
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的TD学习收敛很慢。什么可能有帮助？</summary>

**答案**：几种技术可以加速收敛：

1. **增加学习率 $\alpha$**：更快更新，但太高会导致不稳定
2. **使用资格迹（TD(λ)）**：一次将信用传播到多个过去状态
3. **更好的初始化**：如果可能，以接近真实值开始
4. **优先更新**：首先更新TD误差最大的状态（优先扫描）
5. **使用n步回报**：在TD(0)和MC之间平衡以获得更好的偏差-方差权衡

**解释**：TD一次传播一步的值。在长链中，信息需要很多回合才能从目标传播到起点。资格迹和n步回报让信息"跳过"多个状态。

**常见陷阱**：设置 $\alpha$ 太高。这会导致振荡和发散。从 $\alpha = 0.1$ 或更小开始。
</details>

<details markdown="1">
<summary><strong>问题6（概念）：</strong> 什么时候应该使用DP vs MC vs TD？</summary>

**答案**：这取决于你可以访问什么：

| 情况 | 最佳方法 |
|-----------|-------------|
| 有模型（$P$, $R$） | 动态规划 |
| 无模型，回合制任务 | 蒙特卡洛或TD |
| 无模型，持续任务 | TD（MC需要回合） |
| 需要低方差 | TD（带资格迹） |
| 需要无偏估计 | 蒙特卡洛 |
| 需要在线学习 | TD |

**解释**：
- **DP** 在有模型时最有效，但模型通常不可用
- **MC** 是无偏的但高方差，需要完整回合
- **TD** 最灵活——在线工作，适用于持续任务，方差更低

**常见陷阱**：对持续任务使用MC（无限回合）或在没有模型的情况下尝试DP。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第4-6章
- **Szepesvári (2010)**, 强化学习算法
- **Silver's RL Course**, 第3-4讲：动态规划和无模型预测

**面试需要记忆的**：三种方法（DP/MC/TD），自举定义，偏差-方差权衡，为什么贝尔曼方程使RL可行，收敛的压缩性质。
