# 贝尔曼方程

## 面试摘要

**贝尔曼方程** 递归地表达价值函数：一个状态的价值等于即时奖励加上后继状态的折扣价值。**贝尔曼期望方程** 对任何策略 \(\pi\) 成立。**贝尔曼最优方程** 对最优 \(V^*\) 和 \(Q^*\) 成立。这些方程是RL的基础——动态规划、TD学习和Q学习都利用了它们。

**需要记忆的**：贝尔曼期望的两种形式（用于 \(V^\pi\) 和 \(Q^\pi\)），贝尔曼最优方程，备份图。

---

## 核心定义

### \(V^\pi\) 的贝尔曼期望方程

$$V^\pi(s) = \sum_a \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]$$

**含义**：状态 \(s\) 的价值是期望即时奖励加上下一状态的折扣期望价值，其中期望是对动作（来自策略）和转移（来自环境）求取的。

### \(Q^\pi\) 的贝尔曼期望方程

$$Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q^\pi(s', a')$$

**含义**：在状态 \(s\) 采取动作 \(a\) 的价值等于即时奖励加上下一状态-动作对的折扣期望Q值。

### \(V^*\) 的贝尔曼最优方程

$$V^*(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right]$$

**含义**：最优价值通过采取最佳动作实现（max代替对策略的期望）。

### \(Q^*\) 的贝尔曼最优方程

$$Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s', a')$$

**含义**：最优Q值等于即时奖励加上下一状态的折扣最优价值。

---

## 数学与推导

### 推导 \(V^\pi\) 的贝尔曼期望方程

从定义开始：

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

$$= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]$$

$$= \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s]$$

展开期望：

$$= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) R(s,a,s') + \gamma \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) V^\pi(s')$$

简化（假设 \(R(s,a)\) 不依赖于 \(s'\)）：

$$= \sum_a \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right]$$

### 矩阵形式（线性系统）

对于固定策略，贝尔曼方程在 \(V^\pi\) 中是线性的：

$$\mathbf{V}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi$$

求解：

$$\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{R}^\pi$$

**复杂度**：\(O(|S|^3)\) — 只对小状态空间可行。

### 为什么最优使用Max

对于最优策略，我们总是选择最佳动作：

$$\pi^*(a|s) = \begin{cases} 1 & \text{如果 } a = \arg\max_a Q^*(s,a) \\ 0 & \text{否则} \end{cases}$$

所以对策略的期望变成了max：

$$V^*(s) = \sum_a \pi^*(a|s) Q^*(s,a) = \max_a Q^*(s,a)$$

---

## 算法概述

贝尔曼方程支持关键算法：

| 算法 | 使用 | 关键思想 |
|-----|------|---------|
| 策略评估 | \(V^\pi\) 的贝尔曼期望 | 迭代直到收敛 |
| 策略迭代 | 两个方程 | 交替评估和改进 |
| 值迭代 | \(V^*\) 的贝尔曼最优 | 用max算子迭代 |
| Q学习 | \(Q^*\) 的贝尔曼最优 | 基于采样的max更新 |
| SARSA | \(Q^\pi\) 的贝尔曼期望 | 基于采样的在策略更新 |

### 备份图

**V函数备份**（期望）：
```
      (s)
     / | \
    a  a  a     ← 对动作求和（按π加权）
   /|\ |\ |\
  s' s' s' ...  ← 对下一状态求和（按P加权）
```

**Q函数备份**（最优）：
```
    (s,a)
    / | \
   s' s' s'     ← 对下一状态求和（按P加权）
   |  |  |
   max a'       ← 对下一动作取max
```

---

## 常见陷阱

1. **混淆期望与最优方程**：期望使用策略 \(\pi\) 加权；最优使用max。使用错误的会导致算法不正确。

2. **忘记自引用性质**：贝尔曼方程用 \(V\) 定义 \(V\) —— 它们是不动点方程，不是直接公式。

3. **矩阵求逆的规模**：封闭形式解 \((\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1}\) 是 \(O(|S|^3)\) — 对大状态空间不可行。

4. **假设确定性转移**：方程包含 \(\sum_{s'} P(s'|s,a)\) — 不要忘记转移的随机性。

---

## 小例子

**两状态MDP：**

- 状态：\(S = \{A, B\}\)
- 动作：\(A = \{0, 1\}\)（两个状态中效果相同）
- 转移：从A，动作0 → 留在A，动作1 → 去B。从B，任何动作 → 留在B。
- 奖励：\(R(A, 0) = 1\), \(R(A, 1) = 0\), \(R(B, \cdot) = 2\)
- \(\gamma = 0.9\)

**策略**：总是动作0。

**\(V^\pi(A)\) 的贝尔曼方程**：

$$V^\pi(A) = R(A, 0) + \gamma \cdot 1 \cdot V^\pi(A) = 1 + 0.9 V^\pi(A)$$

$$V^\pi(A) = \frac{1}{1-0.9} = 10$$

**\(V^*(A)\) 的贝尔曼最优方程**：

$$V^*(A) = \max\{1 + 0.9 V^*(A), \; 0 + 0.9 V^*(B)\}$$

因为 \(V^*(B) = \frac{2}{1-0.9} = 20\)：

$$V^*(A) = \max\{10, \; 0 + 0.9 \times 20\} = \max\{10, 18\} = 18$$

**洞见**：最优策略是去B（动作1），因为在留下的情况下 \(V^*(B) > V^*(A)\)。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 贝尔曼期望方程和贝尔曼最优方程的关键区别是什么？</summary>

**答案**：
- **期望**：使用策略 \(\pi(a|s)\) 来加权动作 — 描述给定策略下的价值
- **最优**：对动作使用 \(\max_a\) — 描述最优策略下的价值

**解释**：期望方程评估任何固定策略。最优方程找到最佳可能行为。策略迭代使用期望（来评估），然后改进（使用max）。值迭代直接使用最优方程。

**关键方程**：
- 期望：\(V^\pi(s) = \sum_a \pi(a|s)[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')]\)
- 最优：\(V^*(s) = \max_a[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s')]\)

**常见陷阱**：当你想评估一个特定的（非最优）策略时使用最优方程。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么贝尔曼方程被称为"递归"或"自引用"的？</summary>

**答案**：一个状态的价值是用其他状态（后继者）的价值来定义的，而这些状态又是以同样方式定义的。

**解释**：\(V(s)\) 出现在方程的两边。这不是错误——它是一个不动点方程。我们通过迭代（动态规划）或采样（TD学习）来求解它。递归在终止状态处终止，其中 \(V(s_{终止}) = 0\)。

**关键方程**：\(V(s) = R + \gamma V(s')\) — \(V\) 用 \(V\) 定义。

**常见陷阱**：认为你可以不通过迭代直接计算 \(V(s)\)。方程是一个约束，不是公式。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 写出Q*(s,a)的贝尔曼方程并解释每一项。</summary>

**答案**：

$$Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \max_{a'} Q^*(s',a')$$

**解释**：
- \(R(s,a)\)：在状态 \(s\) 采取动作 \(a\) 的即时奖励
- \(\gamma\)：折扣因子（我们多重视未来）
- \(\sum_{s'} P(s'|s,a)\)：对随机转移的期望
- \(\max_{a'} Q^*(s',a')\)：下一状态的最佳动作价值（最优性）

**常见陷阱**：在Q学习中，我们采样 \(s'\) 而不是求和。更新变成：\(Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\)
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 为什么我们可以封闭形式求解V^π但不能求解V*？</summary>

**答案**：\(V^\pi\) 满足线性系统（可以求逆矩阵）。\(V^*\) 有 \(\max\) 算子，使其非线性。

**解释**：
- 贝尔曼期望：\(\mathbf{V}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi \Rightarrow \mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{R}^\pi\)
- 贝尔曼最优：\(\max\) 阻止了矩阵形式。我们必须使用迭代方法（值迭代）。

**关键洞见**：这就是为什么策略迭代要交替——评估（线性）可以精确求解，但找到最佳策略需要max。

**常见陷阱**：试图"解析求解" \(V^*\) — 它需要迭代。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的值迭代不收敛。可能出了什么问题？</summary>

**答案**：常见问题：
1. 连续任务中 \(\gamma = 1\)（没有收敛保证）
2. 转移概率有错误（和不为1）
3. 奖励量级太大（数值不稳定）
4. 迭代不够（收敛可能很慢）
5. 使用错误的贝尔曼方程（期望而非最优）

**解释**：当 \(\gamma < 1\) 时值迭代收敛，因为贝尔曼算子是压缩映射。检查：(1) 终止状态价值为0，(2) \(\gamma < 1\)，(3) 转移是正确的概率分布，(4) 监控 \(\max_s |V_{k+1}(s) - V_k(s)|\) 的收敛。

**常见陷阱**：设置收敛阈值太紧（如 \(10^{-10}\)）导致不必要的迭代。实际阈值通常是 \(10^{-4}\) 或基于策略稳定性。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第3.5章，4.1章
- **Bellman (1957)**, 动态规划
- **Bertsekas & Tsitsiklis**, 神经动态规划，第2章

**面试需要记忆的**：所有四个贝尔曼方程（V和Q的期望和最优），递归结构，为什么最优使用max，收敛的压缩映射直觉。
