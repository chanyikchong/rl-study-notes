# MDP基础

## 面试摘要

**马尔可夫决策过程（MDP）** 是序贯决策的数学框架。关键组成：状态 \(S\)、动作 \(A\)、转移动态 \(P(s'|s,a)\)、奖励 \(R(s,a,s')\)、折扣因子 \(\gamma\)。马尔可夫性质指出未来只依赖当前状态，与历史无关。MDP是所有RL算法的基础——需要彻底理解。

**需要记忆的**：MDP元组 \((S, A, P, R, \gamma)\)，马尔可夫性质定义，回报公式 \(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)。

---

## 核心定义

### MDP元组

MDP由元组 \((S, A, P, R, \gamma)\) 定义：

| 组成部分 | 符号 | 描述 |
|---------|------|-----|
| 状态空间 | \(S\) | 所有可能状态的集合 |
| 动作空间 | \(A\) | 所有可能动作的集合（可依赖状态 \(A(s)\)）|
| 转移函数 | \(P(s' \mid s,a)\) | 给定状态 \(s\) 和动作 \(a\) 时到达 \(s'\) 的概率 |
| 奖励函数 | \(R(s,a,s')\) 或 \(R(s,a)\) | 即时奖励信号 |
| 折扣因子 | \(\gamma \in [0,1]\) | 未来奖励的重视程度 |

### 马尔可夫性质

$$P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \ldots, S_0, A_0) = P(S_{t+1} | S_t, A_t)$$

**含义**：转移到任何未来状态的概率只依赖于当前状态和动作，与如何到达当前状态的历史无关。

### 轨迹与回合

**轨迹**（或回合）是一个序列：

$$\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \ldots)$$

**回合** 在到达终止状态时结束。

---

## 数学与推导

### 回报（累积奖励）

**回报** \(G_t\) 是从时刻 \(t\) 开始的总折扣奖励：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

**为什么要折扣？**
1. 数学便利性：确保无限时间范围内 \(G_t\) 有限
2. 偏好即时奖励（行为现实性）
3. 对未来不确定性的建模

**递归关系**（对TD学习很重要）：

$$G_t = R_{t+1} + \gamma G_{t+1}$$

### \(\gamma\) 的特殊情况

| \(\gamma\) 值 | 行为 |
|--------------|------|
| \(\gamma = 0\) | 短视：只关心即时奖励 |
| \(\gamma = 1\) | 不折扣：所有奖励同等重要（仅对回合任务有效）|
| \(\gamma \to 1\) | 远视：关心长期后果 |

### 期望回报

由于转移是随机的，我们关心**期望回报**：

$$\mathbb{E}[G_t | S_t = s]$$

这个期望是对转移和策略中的随机性求取的。

---

## 算法概述

MDP本身没有"算法"——它们是问题的形式化。目标是找到一个**策略** \(\pi\) 来最大化期望回报。方法包括：

1. **动态规划**（需要已知 \(P\) 和 \(R\)）
2. **蒙特卡洛**（从完整回合学习）
3. **时序差分**（从估计中自举）
4. **深度RL**（使用神经网络处理大状态空间）

---

## 常见陷阱

1. **混淆 \(R_t\) 与 \(R_{t+1}\)**：惯例不同。Sutton & Barto使用 \(R_{t+1}\) 表示在状态 \(S_t\) 执行动作 \(A_t\) 后收到的奖励。

2. **忘记马尔可夫假设**：如果你的状态没有捕获所有相关信息，马尔可夫性质被违反（部分可观测 → POMDP）。

3. **对连续任务使用 \(\gamma = 1\)**：可能导致无限回报。只对有保证终止的回合任务使用。

4. **状态与观测混淆**：实际中，我们常有观测 \(O\) 是真实状态的函数。这对部分可观测性很重要。

---

## 小例子

**网格世界MDP：**

```
+---+---+---+---+
| S |   |   | G |
+---+---+---+---+
|   | X |   |   |
+---+---+---+---+
```

- \(S\)：8个非墙格子（状态）
- \(A = \{\text{上}, \text{下}, \text{左}, \text{右}\}\)
- \(P\)：确定性（或0.8预期方向，0.1每个垂直方向）
- \(R\)：每步-1，目标G处+10，X处-10
- \(\gamma = 0.9\)

**轨迹示例**：S → 右 → 右 → 右 → G 给出回报：\(-1 + 0.9(-1) + 0.9^2(-1) + 0.9^3(10) = 4.39\)

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 什么是马尔可夫性质，为什么它对RL很重要？</summary>

**答案**：马尔可夫性质指出 \(P(S_{t+1}|S_t, A_t) = P(S_{t+1}|S_t, A_t, S_{t-1}, \ldots, S_0)\) —— 给定当前状态，未来与过去独立。

**解释**：这很关键，因为它允许我们只基于当前状态做决策，而不需要记住整个历史。这极大地简化了计算，并使递归贝尔曼方程成为可能。

**关键方程**：\(P(S_{t+1}|S_t, A_t)\)

**常见陷阱**：设计没有捕获所有相关信息的状态违反了马尔可夫性质。例如，在Atari游戏中，单帧不能捕获速度——这就是为什么DQN使用帧堆叠。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 当γ趋近于0和1时，智能体行为有何变化？</summary>

**答案**：
- \(\gamma \to 0\)：智能体变得短视，只最大化即时奖励
- \(\gamma \to 1\)：智能体变得远视，同等重视长期后果

**解释**：折扣因子 \(\gamma\) 控制有效规划范围。当 \(\gamma = 0\) 时，回报只是 \(R_{t+1}\)。当 \(\gamma = 0.99\) 时，100步后的奖励仍然贡献显著（\(0.99^{100} \approx 0.37\)）。

**关键方程**：\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)

**常见陷阱**：对非回合（连续）任务使用 \(\gamma = 1\) 会导致无限回报。对连续任务总是使用 \(\gamma < 1\)。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 证明 G_t = R_{t+1} + γG_{t+1}</summary>

**答案**：直接代数推导。

**解释**：

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$

$$= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \cdots)$$

$$= R_{t+1} + \gamma G_{t+1}$$

**关键方程**：\(G_t = R_{t+1} + \gamma G_{t+1}\)

**常见陷阱**：这个递归关系是TD学习的基础。忘记这个会导致对TD如何从价值估计中自举产生困惑。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 如果 γ = 0.9 且每步奖励永远是+1，G_0是多少？</summary>

**答案**：\(G_0 = 10\)

**解释**：这是一个几何级数：

$$G_0 = \sum_{k=0}^{\infty} \gamma^k \cdot 1 = \frac{1}{1-\gamma} = \frac{1}{1-0.9} = 10$$

**关键方程**：对于恒定奖励 \(r\)：\(G_t = \frac{r}{1-\gamma}\)

**常见陷阱**：忘记这个公式。它对快速健全性检查和理解价值函数的规模很有用。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 如何调试一个看起来随机行动的RL智能体？</summary>

**答案**：按顺序检查：
1. 验证奖励信号被正确接收（打印奖励）
2. 检查状态表示是否捕获了相关信息（马尔可夫性质）
3. 验证折扣因子不是太低（短视行为）
4. 检查探索率（ε可能太高）
5. 确保已经进行了足够的训练步骤

**解释**：随机行为通常表示智能体没有学到任何有用的东西。最常见的原因是奖励错误（没有收到信号）、状态表示差（无法区分情况）或训练不足。

**常见陷阱**：当问题通常在环境设置或奖励函数时，从算法开始调试。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第3章
- **Bellman (1957)**, 动态规划 — MDP的原始表述
- **Puterman (1994)**, 马尔可夫决策过程 — 全面的数学处理

**面试需要记忆的**：MDP元组定义，马尔可夫性质陈述，带折扣的回报公式，递归回报关系。
