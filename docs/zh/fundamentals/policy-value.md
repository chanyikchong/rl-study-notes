# 策略与价值函数

## 面试摘要

**策略** \(\pi(a|s)\) 将状态映射到动作概率。**状态价值函数** \(V^\pi(s)\) 给出从 \(s\) 开始并遵循 \(\pi\) 的期望回报。**动作价值函数** \(Q^\pi(s,a)\) 给出从 \(s\) 开始，采取动作 \(a\)，然后遵循 \(\pi\) 的期望回报。RL的目标是找到**最优策略** \(\pi^*\)，使所有状态的价值最大化。

**需要记忆的**：\(V^\pi(s)\), \(Q^\pi(s,a)\) 的定义，关系式 \(V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)\)，最优价值记号 \(V^*(s)\), \(Q^*(s,a)\)。

---

## 核心定义

### 策略

**策略** \(\pi\) 指定行为：

- **确定性策略**：\(\pi(s) = a\) — 直接将状态映射到动作
- **随机策略**：\(\pi(a|s) = P(A_t = a | S_t = s)\) — 动作上的概率分布

**为什么是随机的？**
1. 可以表示混合策略（博弈论）
2. 在学习过程中支持探索
3. 某些问题需要随机化才能达到最优（对抗性设置）

### 状态价值函数

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s\right]$$

**含义**：从状态 \(s\) 开始并之后遵循策略 \(\pi\) 的期望累积折扣奖励。

### 动作价值函数

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s, A_t = a\right]$$

**含义**：从状态 \(s\) 开始，采取动作 \(a\)，然后遵循策略 \(\pi\) 的期望累积折扣奖励。

### V和Q的关系

$$V^\pi(s) = \sum_{a \in A} \pi(a|s) \, Q^\pi(s, a)$$

**含义**：状态价值是策略动作分布下的期望动作价值。

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')$$

**含义**：动作价值是即时奖励加上下一状态的折扣期望价值。

---

## 数学与推导

### 从Q推导V

从定义开始：

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

$$= \mathbb{E}_\pi[\mathbb{E}_\pi[G_t | S_t = s, A_t] | S_t = s]$$

$$= \sum_a P(A_t = a | S_t = s) \cdot \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$

$$= \sum_a \pi(a|s) \, Q^\pi(s, a)$$

### 最优价值函数

**最优状态价值函数**：

$$V^*(s) = \max_\pi V^\pi(s) = \max_a Q^*(s, a)$$

**最优动作价值函数**：

$$Q^*(s, a) = \max_\pi Q^\pi(s, a)$$

### 最优策略

给定 \(Q^*\)，最优策略是：

$$\pi^*(a|s) = \begin{cases} 1 & \text{如果 } a = \arg\max_{a'} Q^*(s, a') \\ 0 & \text{否则} \end{cases}$$

**关键洞见**：如果我们知道 \(Q^*\)，最优行动很简单——只需选择Q值最高的动作。

### 优势函数

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

**含义**：动作 \(a\) 比在 \(\pi\) 下的平均动作好多少？

**性质**：
- \(\mathbb{E}_{a \sim \pi}[A^\pi(s,a)] = 0\)（优势平均为零）
- \(A^\pi(s, a) > 0\) 表示动作 \(a\) 优于平均
- 在策略梯度方法（A2C, PPO）中大量使用

---

## 算法概述

价值函数本身不是算法，而是**我们要学习的东西**：

1. **策略评估**：给定 \(\pi\)，计算 \(V^\pi\) 或 \(Q^\pi\)
2. **策略改进**：给定 \(V^\pi\) 或 \(Q^\pi\)，构建更好的策略
3. **基于价值的方法**：学习 \(Q^*\)，从argmax推导 \(\pi^*\)
4. **基于策略的方法**：直接优化 \(\pi\) 参数

---

## 常见陷阱

1. **混淆 \(V\) 和 \(Q\)**：
   - \(V(s)\) — 未指定动作，对策略求平均
   - \(Q(s,a)\) — 给定特定动作

2. **忘记期望**：价值是期望回报，不是单次采样回报。

3. **最优 ≠ 对当前估计贪婪**：对 \(Q^\pi\) 贪婪给出改进的策略，但只有对 \(Q^*\) 贪婪才给出最优策略。

4. **不理解Q值何时存在**：Q值对任何策略都有定义，不仅是最优策略。

---

## 小例子

**两状态MDP：**

```
状态A ──动作0 (r=5)──> 终止
    │
    └──动作1 (r=0)──> 状态B ──动作0 (r=10)──> 终止
```

当 \(\gamma = 0.9\) 时：

**对于"总是动作0"的策略：**
- \(V^\pi(A) = 5\), \(V^\pi(B) = 10\)
- \(Q^\pi(A, 0) = 5\), \(Q^\pi(A, 1) = 0 + 0.9 \times 10 = 9\)

**最优策略**：在A采取动作1（到达B），然后在B采取动作0。
- \(V^*(A) = 9\), \(V^*(B) = 10\)

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> V(s)和Q(s,a)有什么区别？</summary>

**答案**：
- \(V^\pi(s)\)：从状态 \(s\) 开始，根据 \(\pi\) 对动作求平均的期望回报
- \(Q^\pi(s,a)\)：从状态 \(s\) 采取特定动作 \(a\) 后的期望回报

**解释**：\(V\) 总结了在一个状态（在某策略下）的价值，而 \(Q\) 评估特定的状态-动作对。它们的关系是 \(V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)\)。

**关键方程**：\(V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)\)

**常见陷阱**：用 \(Q\) 但意思是 \(V\)，或反之。在Q学习中我们学习 \(Q\)；在策略评估中我们通常计算 \(V\)。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么优势函数在策略梯度中有用？</summary>

**答案**：优势 \(A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)\) 减少策略梯度估计的方差，同时保持梯度无偏。

**解释**：策略梯度按回报加权动作。高回报状态会增加所有动作（甚至坏动作）的概率。优势减去基线 \(V^\pi(s)\)，所以只有优于平均的动作被强化。

**关键方程**：\(A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)\)

**常见陷阱**：忘记 \(\mathbb{E}[A^\pi(s,a)] = 0\)。基线不会使梯度有偏，因为它不依赖于动作。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 证明 V*(s) = max_a Q*(s,a)</summary>

**答案**：根据最优性的定义。

**解释**：

$$V^*(s) = \max_\pi V^\pi(s) = \max_\pi \sum_a \pi(a|s) Q^\pi(s,a)$$

对于最优策略，我们贪婪地行动：

$$\pi^*(a|s) = 1 \text{ 如果 } a = \arg\max_{a'} Q^*(s,a')$$

因此：

$$V^*(s) = \sum_a \pi^*(a|s) Q^*(s,a) = Q^*(s, \arg\max_a Q^*(s,a)) = \max_a Q^*(s,a)$$

**关键方程**：\(V^*(s) = \max_a Q^*(s,a)\)

**常见陷阱**：这只对 \(V^*\) 和 \(Q^*\) 成立，对任意策略价值不成立。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 给定Q*(s,a)，如何提取最优策略？</summary>

**答案**：\(\pi^*(s) = \arg\max_a Q^*(s,a)\)

**解释**：一旦我们有了最优动作价值函数，任何状态的最优动作就是Q值最高的动作。这就是为什么Q学习很受欢迎——我们可以直接从学到的Q值推导最优策略，而不需要模型。

**关键方程**：\(\pi^*(s) = \arg\max_a Q^*(s,a)\)

**常见陷阱**：这需要精确知道 \(Q^*\)。在学习过程中，我们有估计 \(\hat{Q}\)，对估计贪婪（利用）必须与探索平衡。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的Q值都是非常大的正数。可能出了什么问题？</summary>

**答案**：可能的问题：
1. 连续任务中折扣因子 \(\gamma\) 太接近1（价值爆炸）
2. 正奖励循环（智能体循环积累奖励）
3. 缺少终止状态处理
4. 奖励规模太大
5. 价值函数发散（致命三角）

**解释**：Q值应该大致反映折扣累积奖励。如果奖励有界（如-1到+1）且 \(\gamma = 0.99\)，最大Q应该在 \(1/(1-0.99) = 100\) 左右。更大的值表示有问题。

**常见陷阱**：不规范化或裁剪奖励。大奖励导致大梯度和不稳定学习。DQN将奖励裁剪到 \([-1, 1]\)。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第3.5-3.8章
- **Szepesvári (2010)**, 强化学习算法
- **Silver的RL课程**, 第2讲：马尔可夫决策过程

**面试需要记忆的**：\(V^\pi\), \(Q^\pi\), \(V^*\), \(Q^*\) 的定义，优势函数，V-Q关系，如何从 \(Q^*\) 提取最优策略。
