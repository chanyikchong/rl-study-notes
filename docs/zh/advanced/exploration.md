# 探索策略

## 面试摘要

**探索** 是采取动作以发现环境新信息的过程。关键张力：**利用**（使用已知好的动作）vs **探索**（尝试未知动作）。常见策略：**ε-贪婪**（以概率ε随机），**softmax/玻尔兹曼**（基于Q值的概率），**UCB**（面对不确定性时乐观），**熵奖励**（奖励策略随机性）。深度探索（跨时间步协调）是活跃的研究领域。

**需要记忆的**：ε-贪婪，UCB公式，熵奖励，探索-利用权衡。

---

## 核心定义

### 探索-利用困境

- **利用**：选择已知好的动作 → 短期最优
- **探索**：尝试不太了解的动作 → 可能发现更好的长期策略

两者都是必要的。纯利用可能陷入局部最优。纯探索不能利用知识。

### ε-贪婪

$$a = \begin{cases} \arg\max_a Q(s,a) & \text{以概率 } 1-\epsilon \\ \text{随机动作} & \text{以概率 } \epsilon \end{cases}$$

**优点**：简单，无额外计算
**缺点**：随机探索（不优先考虑有前途的动作），所有非贪婪动作概率相等

### Softmax（玻尔兹曼）探索

$$P(a|s) = \frac{\exp(Q(s,a) / \tau)}{\sum_{a'} \exp(Q(s,a') / \tau)}$$

其中 \(\tau\) 是温度：
- \(\tau \to 0\)：贪婪（确定性）
- \(\tau \to \infty\)：均匀随机
- \(\tau = 1\)：标准softmax

**优点**：更高价值的动作被更频繁探索

### 置信上界（UCB）

$$a = \arg\max_a \left[ Q(s,a) + c \sqrt{\frac{\ln t}{N(s,a)}} \right]$$

**直觉**：为很少尝试的动作添加"探索奖励"。
- \(t\)：总时间步
- \(N(s,a)\)：在状态 \(s\) 采取动作 \(a\) 的次数
- \(c\)：探索系数

**原则**："面对不确定性时乐观"

---

## 数学与推导

### UCB遗憾界

对于多臂老虎机设置，UCB实现：

$$\text{遗憾}(T) = O(\sqrt{KT \ln T})$$

其中 \(K\) 是臂的数量。这是接近最优的。

### 策略梯度中的熵奖励

将熵添加到目标：

$$J(\theta) = \mathbb{E}_{\pi_\theta}[R] + \beta H(\pi_\theta)$$

其中：

$$H(\pi_\theta(s)) = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$$

**效果**：惩罚确定性策略，鼓励探索。

**梯度**：

$$\nabla_\theta H = -\sum_a (\log \pi_\theta(a|s) + 1) \nabla_\theta \pi_\theta(a|s)$$

### 内在动机

为访问新颖状态添加内在奖励：

$$r^{total} = r^{extrinsic} + \beta \cdot r^{intrinsic}$$

常见内在奖励：
- **好奇心**（ICM）：下一状态的预测误差
- **基于计数**：\(1/\sqrt{N(s)}\) 用于访问计数
- **RND**：随机网络预测误差

---

## 算法概述

### 带衰减的ε-贪婪

```
算法：带线性衰减的ε-贪婪

初始：ε_start = 1.0, ε_end = 0.01, decay_steps = 100000

对每步 t：
    ε = max(ε_end, ε_start - (ε_start - ε_end) * t / decay_steps)
    以概率 ε：
        a = 随机动作
    否则：
        a = argmax_a Q(s, a)
```

### 老虎机的UCB

```
算法：UCB1

初始化：N(a) = 0, Q(a) = 0 对所有动作

对 t = 1 到 T：
    如果任何动作的 N(a) = 0：
        选择那个动作
    否则：
        a = argmax_a [Q(a) + c * sqrt(ln(t) / N(a))]

    观察奖励 r
    N(a) ← N(a) + 1
    Q(a) ← Q(a) + (r - Q(a)) / N(a)
```

---

## 常见陷阱

1. **ε从不衰减**：智能体持续犯随机错误。训练中衰减ε。

2. **ε衰减太快**：早期探索不足。从ε = 1.0开始，缓慢衰减。

3. **忽视状态相关探索**：ε-贪婪均匀探索；一些状态需要更多探索。

4. **熵系数太高/太低**：太高 → 随机策略。太低 → 没有探索。

5. **带函数逼近的UCB**：基于计数的UCB在大状态空间难以使用。需要伪计数或神经方法。

---

## 小例子

**10臂老虎机：**

真实均值：[0.1, 0.5, 0.3, 0.7, 0.2, 0.4, 0.6, 0.8, 0.9, 0.35]
最优臂：9（均值 = 0.9）

1000步后：
- **ε-贪婪 (ε=0.1)**：遗憾 ≈ 50，采样所有臂但在坏臂上浪费时间
- **UCB**：遗憾 ≈ 30，快速聚焦于好臂同时验证
- **纯贪婪**：遗憾变化很大，可能锁定在次优臂

**关键洞见**：UCB通过优先考虑不确定的臂来高效探索。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 什么是探索-利用权衡？</summary>

**答案**：以下之间的张力：
- **利用**：使用当前知识最大化即时奖励
- **探索**：采取不确定的动作获取信息

**解释**：学习早期，探索至关重要——你不知道什么是最好的。后期，利用变得更有价值——你应该使用学到的东西。最优平衡取决于时间范围（还剩多少时间）。

**关键洞见**：在有限时间范围设置中，探索应该随时间减少。

**常见陷阱**：总是使用固定ε——不适应学习进度。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 为什么softmax（玻尔兹曼）对某些问题比ε-贪婪更好？</summary>

**答案**：Softmax更频繁探索有前途的动作，而ε-贪婪平等对待所有非贪婪动作。

**解释**：用ε-贪婪，如果Q值是[10, 9, 1, 1]，三个非贪婪动作概率相等。Softmax使9动作比1动作更可能被选中。

**权衡**：Softmax需要计算指数且对Q值尺度敏感。

**常见陷阱**：使用错误的温度τ。太低 → 像贪婪。太高 → 像随机。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 从置信区间推导UCB公式的直觉。</summary>

**答案**：UCB将置信上界添加到估计价值上。

**推导直觉**：
- \(Q(a)\) 是动作 \(a\) 奖励的样本均值
- 根据Hoeffding不等式，真实均值以概率 \(1-\delta\) 在 \(\sqrt{\frac{\ln(1/\delta)}{2N(a)}}\) 范围内
- 设 \(\delta = 1/t\) 给出 \(\sqrt{\frac{\ln t}{N(a)}}\) 奖励
- 我们假设真实价值在上界（乐观）

**关键方程**：\(a = \arg\max [Q(a) + c\sqrt{\frac{\ln t}{N(a)}}]\)

**常见陷阱**：在RL中使用UCB而不适应自举和函数逼近。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 熵正则化如何鼓励探索？</summary>

**答案**：将 \(\beta H(\pi)\) 添加到目标奖励高熵（更随机）的策略。

$$H(\pi(s)) = -\sum_a \pi(a|s) \log \pi(a|s)$$

**效果**：
- 确定性策略：H = 0
- 均匀策略：H = log|A|（最大值）

通过与奖励一起最大化H，我们防止过早收敛到确定性策略。

**常见陷阱**：β太大使策略忽略奖励；β太小没有效果。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的RL智能体学习缓慢，似乎错过了好动作。如何改善探索？</summary>

**答案**：尝试的策略：
1. **增加初始ε**：从1.0开始，训练中衰减到0.01
2. **添加熵奖励**：在策略梯度目标中使用β ≈ 0.01
3. **尝试UCB风格奖励**：为Q值添加乐观
4. **使用好奇心/内在动机**：ICM、RND用于稀疏奖励环境
5. **多样经验收集**：不同种子的多个并行环境
6. **参数噪声**：对网络权重添加噪声而不是动作

**解释**：正确的方法取决于环境。密集奖励 → ε-贪婪通常足够。稀疏奖励 → 需要内在动机。

**常见陷阱**：假设一种探索策略适用于所有问题。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第2章
- **Auer et al. (2002)**, 多臂老虎机问题的有限时间分析（UCB）
- **Pathak et al. (2017)**, 好奇心驱动的探索（ICM）
- **Burda et al. (2019)**, 通过随机网络蒸馏探索

**面试需要记忆的**：ε-贪婪，softmax，UCB公式，熵奖励，内在动机概念。
