# 深度RL中的稳定性问题

## 面试摘要

深度RL可能不稳定：Q值发散，策略崩溃，或性能振荡。**致命三角**（函数逼近+自举+异策略）解释了许多失败。关键稳定性技术：**目标网络**（冻结的Q用于目标），**经验回放**（打破相关性），**奖励裁剪**，**梯度裁剪**。理解事情何时以及为何失败对调试至关重要。

**需要记忆的**：致命三角组成部分，为什么每个导致问题，常见稳定化技术。

---

## 核心定义

### 致命三角

三个元素一起可能导致发散：

1. **函数逼近**：跨状态泛化
2. **自举**：使用价值估计作为目标（TD学习）
3. **异策略学习**：从另一个策略的数据学习

**关键洞见**：每个元素单独是没问题的。是组合才致命。

### 发散类型

1. **价值发散**：Q(s,a) → ±∞
2. **策略崩溃**：π在坏动作上变得确定性
3. **灾难性遗忘**：智能体忘记如何解决任务的早期部分

### 过估计偏差

$$\mathbb{E}[\max_a Q(s,a)] \geq \max_a \mathbb{E}[Q(s,a)]$$

Q估计中有噪声时，max选择过估计的值。这通过自举累积。

---

## 数学与推导

### 为什么三角导致问题

**函数逼近 + 自举**：
- 基于 \(V(s')\) 更新状态 \(s\)
- 但更新参数也影响 \(V(s')\)（泛化）
- 可以创建正反馈循环：\(V(s') \uparrow \Rightarrow V(s) \uparrow \Rightarrow V(s') \uparrow\)

**异策略 + 自举**：
- 目标 \(V(s')\) 是为与数据不同的策略
- 更新可能不收敛到任何有意义的东西

**异策略 + 函数逼近**：
- 数据分布与目标策略的状态分布不匹配
- 一些状态代表过度，其他代表不足
- 泛化放大代表不足区域的误差

### 反例：Baird星

线性FA和TD发散的经典例子：
```
     *
    /|\
   / | \
  *  *  *  （5个外部状态）
     |
     *     （1个中心状态）
```

异策略采样时，Q值发散到无穷大尽管奖励有界。

### 目标网络分析

没有目标网络：

$$\theta \leftarrow \theta + \alpha (r + \gamma \max_a Q(s',a;\theta) - Q(s,a;\theta)) \nabla_\theta Q(s,a;\theta)$$

目标每步都变化 → 追逐移动目标。

有目标网络 \(\theta^-\)：

$$\theta \leftarrow \theta + \alpha (r + \gamma \max_a Q(s',a;\theta^-) - Q(s,a;\theta)) \nabla_\theta Q(s,a;\theta)$$

目标固定C步 → 稳定的回归目标。

---

## 算法概述

### 稳定性技术总结

| 技术 | 作用 | 帮助什么 |
|-----------|--------------|------------|
| 目标网络 | 冻结目标Q | 自举稳定性 |
| 经验回放 | 随机采样 | 打破相关性 |
| Double DQN | 分离选择/评估 | 过估计 |
| 对决架构 | 分离V和A | 价值估计 |
| 优先回放 | 关注高误差 | 样本效率 |
| 奖励裁剪 | 限制奖励到[-1,1] | 梯度大小 |
| 梯度裁剪 | 限制梯度范数 | 梯度爆炸 |
| 层归一化 | 归一化激活 | 训练稳定性 |

### 诊断不稳定

```
调试清单：

1. Q值爆炸？
   → 检查奖励尺度，添加奖励裁剪
   → 降低学习率
   → 添加目标网络（如果没有使用）

2. Q值都相似？
   → 可能欠拟合；增加容量
   → 检查环境是否给出不同奖励

3. 策略熵崩溃？
   → 添加熵奖励
   → 检查softmax中的温度

4. 性能振荡？
   → 降低学习率
   → 增加目标更新周期
   → 使用更大回放缓冲区

5. 突然性能下降？
   → 灾难性遗忘；更大回放缓冲区
   → 检查done/终止处理的bug
```

---

## 常见陷阱

1. **没有目标网络**：对DQN稳定性至关重要。每1K-10K步更新。

2. **回放缓冲区太小**：需要多样性。使用100K-1M转移。

3. **学习率太高**：深度RL敏感。从1e-4开始。

4. **奖励尺度太大**：导致大梯度。裁剪或归一化奖励。

5. **不裁剪梯度**：大梯度 → 大参数变化 → 不稳定。

6. **将同策略方法用作异策略**：策略梯度方法需要新鲜数据。

---

## 小例子

**发散演示：**

简单2状态MDP带线性FA：
- 状态：s₁, s₂
- 特征：φ(s₁) = [1, 0], φ(s₂) = [0, 1]
- 权重：w = [w₁, w₂]
- 转移：s₁ → s₂ → s₁（确定性循环）
- 奖励：r = 0 总是
- γ = 0.99

异策略采样（总是从s₁开始）：
```
w₁ 基于 V(s₂) = w₂ 更新
w₂ 基于 V(s₁) = w₁ 更新
```

如果w₁稍微过估计：
- w₂增加来匹配
- w₁进一步增加
- 正反馈 → 发散

**解决方案**：同策略采样或目标网络。

---

## 测验

<details markdown="1">
<summary><strong>问题1（概念）：</strong> 解释致命三角以及为什么每个组成部分导致不稳定。</summary>

**答案**：

1. **函数逼近**：权重变化影响许多状态。对状态s的更新到处改变价值，可能有害。

2. **自举**：目标依赖当前估计。如果估计错误，我们向错误值回归。误差复合。

3. **异策略**：数据来自与我们学习的不同策略。一些状态-动作对代表不足，导致差的估计，泛化放大这些。

**一起**：误差传播（自举），影响不相关状态（FA），无法通过数据分布修正（异策略）。

**常见陷阱**：认为任何单个元素是问题。是组合。
</details>

<details markdown="1">
<summary><strong>问题2（概念）：</strong> 目标网络如何防止不稳定？</summary>

**答案**：目标网络将TD目标冻结C步，使学习像带固定标签的监督回归。

**解释**：没有目标网络：
- 目标：r + γ max Q(s', a'; θ)
- 每次梯度步改变θ，改变目标
- 像追逐移动目标

有目标网络：
- 目标：r + γ max Q(s', a'; θ⁻)
- θ⁻固定C步
- 我们向稳定值回归

**常见陷阱**：更新目标太频繁。C应该大（1000-10000步）。
</details>

<details markdown="1">
<summary><strong>问题3（数学）：</strong> 数学解释过估计偏差。</summary>

**答案**：对于随机变量X₁, ..., Xₙ：

$$\mathbb{E}[\max_i X_i] \geq \max_i \mathbb{E}[X_i]$$

**应用到Q学习**：Q(s,a)有估计噪声。当我们计算max_a Q(s,a)时，我们倾向于选择有向上噪声的动作。这个选择偏差意味着我们过估计。

**自举复合它**：过估计的Q(s') → 过估计的目标 → 过估计的Q(s) → ...

**解决方案**：Double DQN解耦选择（哪个动作）和评估（它的价值是什么）：

$$y = r + \gamma Q(s', \arg\max_a Q(s',a;\theta); \theta^-)$$

**常见陷阱**：实践中忽视这个。总是使用Double DQN。
</details>

<details markdown="1">
<summary><strong>问题4（数学）：</strong> 为什么异策略学习理论上需要重要性采样？</summary>

**答案**：我们想要估计：

$$V^\pi(s) = \mathbb{E}_{a \sim \pi}[Q^\pi(s,a)]$$

但我们的数据有来自行为策略b的动作：

$$\mathbb{E}_{a \sim b}[Q(s,a)] \neq V^\pi(s)$$

通过重要性采样修正：

$$\mathbb{E}_{a \sim b}\left[\frac{\pi(a|s)}{b(a|s)} Q(s,a)\right] = V^\pi(s)$$

**Q学习的技巧**：使用max而不是对π的期望。这绕过重要性采样但仍有问题（致命三角）。

**常见陷阱**：Q学习在表格情况下不需要重要性权重收敛，但FA会出问题。
</details>

<details markdown="1">
<summary><strong>问题5（实践）：</strong> 你的DQN训练显示Q值无界增长。如何修复？</summary>

**答案**：逐步调试：

1. **检查奖励**：它们有界吗？裁剪到[-1, 1]
2. **目标网络**：在使用吗？每10K步更新，不是每步
3. **学习率**：降到1e-5到1e-4
4. **Double DQN**：实现以减少过估计
5. **梯度裁剪**：将梯度裁剪到范数10
6. **网络架构**：如果过拟合减少大小
7. **终止状态**：确保V(terminal) = 0

**解释**：发散的Q值通常意味着自举误差在复合。目标网络和Double DQN是最重要的修复。

**常见陷阱**：当算法根本不稳定时试图用更多数据修复。
</details>

---

## 参考文献

- **Sutton & Barto**, 强化学习：导论，第11章
- **Tsitsiklis & Van Roy (1997)**, 带函数逼近的TD学习分析
- **Van Hasselt et al. (2018)**, 深度RL和致命三角
- **Baird (1995)**, 残差算法（著名反例）

**面试需要记忆的**：致命三角组成部分，目标网络目的，过估计偏差，Double DQN，常见稳定性技术。
